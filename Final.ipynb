{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scripts.utils.nlp_utils as nlp\n",
    "import scripts.utils.grammar as gra\n",
    "import scripts.utils.string_handling as string_hand\n",
    "import scripts.utils.data_handler as data_hand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#test_data = data_hand.read_test_data(\"textProcessing_testKaldi.csv\")\n",
    "test_data = data_hand.read_test_data(\"kaldi_test_data_real_v1.tsv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Reference Grammar and Diff Grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reference_grammar = gra.read_grammar_and_create_map('referenceGrammar.xml')\n",
    "diff_grammar = gra.read_grammar_and_create_map('diff_rg_1.xml')\n",
    "grammar = gra.merge_grammars(reference_grammar, diff_grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "string_hand.clear_sentence(\"no i will still have a ticket for billy elliot please\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meaning Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def is_slice_in_list(s,l):\n",
    "    len_s = len(s) #so we don't recompute length of s on every iteration\n",
    "    return any(s == l[i:len_s+i] for i in range(len(l) - len_s+1))\n",
    "\n",
    "def is_slice_in_list2(s,l):\n",
    "    len_s = len(s) #so we don't recompute length of s on every iteration\n",
    "    for i in range(len(l) - len_s+1):\n",
    "        if s==l[i:len_s+i]:\n",
    "            return i\n",
    "        \n",
    "def extract_by_pattern(patterns, tags, words):\n",
    "    extracted_words = []\n",
    "    tags_string = \" \".join(tags)\n",
    "    for pattern in patterns:\n",
    "        pattern_string = \" \".join(pattern)\n",
    "        if is_slice_in_list(pattern, tags):\n",
    "            pattern_start_index = is_slice_in_list2(pattern, tags)\n",
    "            if len(pattern) > 1:\n",
    "                #pattern_end_index = tags.index(pattern[-1], pattern_start_index+1)\n",
    "                pattern_end_index = pattern_start_index + len(pattern) -1\n",
    "                if pattern_end_index - pattern_start_index + 1 == len(pattern):\n",
    "                        extracted_part = \"\"\n",
    "                        for i in range(pattern_start_index, pattern_end_index+1):\n",
    "                            extracted_part += words[i] + \" \"\n",
    "                        extracted_words.append(extracted_part[:-1])\n",
    "            else:\n",
    "                extracted_words.append(words[pattern_start_index])\n",
    "            \"\"\"\n",
    "            if len(pattern) > 1:\n",
    "                pattern_end_index = tags.index(pattern[-1], pattern_start_index+1)\n",
    "                if pattern_end_index - pattern_start_index +1 != len(pattern):\n",
    "                    pattern_end_index = tags.index(pattern[-1], pattern_end_index+1)\n",
    "                    \n",
    "                if pattern_end_index - pattern_start_index + 1 == len(pattern):\n",
    "                    extracted_part = \"\"\n",
    "                    for i in range(pattern_start_index, pattern_end_index+1):\n",
    "                        extracted_part += words[i] + \" \"\n",
    "                        \n",
    "\n",
    "                    extracted_words.append(extracted_part[:-1])\n",
    "            else:\n",
    "                extracted_words.append(words[pattern_start_index])\n",
    "            \"\"\"\n",
    "    return extracted_words\n",
    "    \n",
    "\n",
    "def extract_key_dt_nouns(nlp_sent):\n",
    "    tags = nlp_sent[2]\n",
    "    words = nlp.spacy_words_to_string_array(nlp_sent[0])\n",
    "    sentence = \" \".join(words)\n",
    "    #words = \" \".join(nlp_sent[0])\n",
    "    #[['DT', 'NN', 'NN']]\n",
    "    prio_one_patterns = [['DT', 'NN', 'NN'], ['DT', 'NN', 'NNS'],\n",
    "                         ['DT', 'NN']]\n",
    "\n",
    "    extracted_words = extract_by_pattern(prio_one_patterns, tags, words)\n",
    "    return extracted_words\n",
    "    \n",
    "def extract_key_nouns(nlp_sent):\n",
    "    tags = nlp_sent[2]\n",
    "    words = nlp.spacy_words_to_string_array(nlp_sent[0])\n",
    "    sentence = \" \".join(words)\n",
    "    #words = \" \".join(nlp_sent[0])\n",
    "    #[['DT', 'NN', 'NN']]\n",
    "    prio_one_patterns = [['PRP$', 'NN', 'NN'], ['PRP$', 'NN', 'NNS'], ['PRP$', 'NNS', 'NNS'],\\\n",
    "                         ['PRP$', 'NN'], ['PRP$', 'NNS'], ['NN', 'NNS'], ['NN', 'NN'], ['RB', 'NN'], ['JJ', 'NNS'],\\\n",
    "                         ['JJ', 'NN'], ['NN'], ['NNS']]\n",
    "    \n",
    "    # Only when no prio_one_pattern fits\n",
    "    prio_sec_patterns = [['NN'], ['NNS']]\n",
    "    extracted_words = extract_by_pattern(prio_one_patterns, tags, words)\n",
    "    if len(extracted_words) < 1:\n",
    "        extracted_words = extract_by_pattern(prio_sec_patterns, tags, words)\n",
    "    return extracted_words\n",
    "\n",
    "\n",
    "def generate_dt_nouns_by_key_nouns(nlp_nouns):\n",
    "    dts = ['a', 'the']\n",
    "    patterns = [['NN', 'NN'], ['NN', 'NNS'], ['NN']]\n",
    "    tags = nlp_nouns[2]\n",
    "    words = nlp.spacy_words_to_string_array(nlp_nouns[0])\n",
    "    \n",
    "    generated_words = []\n",
    "    for pattern in patterns:\n",
    "        if pattern == tags:\n",
    "            for dt in dts:\n",
    "                sentence = dt + \" \" +\" \".join(words)\n",
    "                generated_words.append(sentence)\n",
    "    return generated_words\n",
    "\n",
    "def generalise_aux_verb(nlp_sent):\n",
    "    tags = nlp_sent[2]\n",
    "    words = nlp.spacy_words_to_string_array(nlp_sent[0])\n",
    "    sentence = \" \".join(words)\n",
    "    patterns = [['PRP', 'MD', 'VB', 'TO'], ['PRP', 'MD', 'VB', 'DT'], ['PRP', 'VBP', 'TO']]\n",
    "\n",
    "    extracted_words = extract_by_pattern(patterns, tags, words)\n",
    "    return extracted_words \n",
    "    \n",
    "def create_meaning_map(grammar):\n",
    "    prompt_noun_map = {}\n",
    "    for prompt in grammar:\n",
    "        try:\n",
    "            extracted_nouns = []\n",
    "            for response in grammar[prompt]:\n",
    "                nlp_s = nlp.nlp_sentence(response)\n",
    "                nouns = extract_key_nouns(nlp_s)\n",
    "                if len(nouns) > 0:\n",
    "                    extracted_nouns.extend(nouns)\n",
    "            prompt_noun_map[prompt] = list(set(extracted_nouns))\n",
    "        except:\n",
    "            print(prompt)\n",
    "    return prompt_noun_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Aux verbs from total reference grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "found_aux_verb = []\n",
    "for key in grammar:\n",
    "    for item in grammar[key]:\n",
    "        nlp_s = nlp.nlp_sentence(item)\n",
    "        aux_verb = generalise_aux_verb(nlp_s)\n",
    "        if len(aux_verb) > 0:\n",
    "            found_aux_verb.extend(aux_verb)\n",
    "print(len(set(found_aux_verb)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list(set(found_aux_verb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "generalise_aux_verb(nlp.nlp_sentence(\"I wish to pay by card\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prompt_noun_map = create_meaning_map(grammar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply Reference Grammar And Preprocessing And Unqiue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "false_counter = 0\n",
    "correct_counter = 0\n",
    "\n",
    "false_prompts = {}\n",
    "safe_prompts = []\n",
    "for prompt_unit in test_data:\n",
    "    \n",
    "    for dict_prompt in test_data[prompt_unit]: \n",
    "        transcript = dict_prompt['transcript']\n",
    "        sentence = transcript\n",
    "        if \"***\" in sentence:\n",
    "            sentence = sentence.replace(\"***\", \"\")\n",
    "        \n",
    "        processed = sentence\n",
    "        \n",
    "        \n",
    "        try:\n",
    "            processed = string_hand.clear_sentence(transcript)\n",
    "        except:\n",
    "            print(dict_prompt)\n",
    "\n",
    "\n",
    "        unique_sentence = string_hand.get_unique_sentence(sentence)\n",
    "        if dict_prompt['id'] == '3796':\n",
    "            print(transcript)\n",
    "            print(unique_sentence)\n",
    "\n",
    "        if sentence not in grammar[prompt_unit] and  \\\n",
    "            processed not in grammar[prompt_unit] and \\\n",
    "            unique_sentence not in grammar[prompt_unit]:\n",
    "                \n",
    "            item = {\"id\": dict_prompt['id'], \"prompt\": str(prompt_unit),\"transcript\": transcript, \"processed\": processed, \"unique\": unique_sentence}\n",
    "            false_counter += 1\n",
    "            if prompt_unit in false_prompts:\n",
    "                false_prompts[prompt_unit].append(item)\n",
    "            else:\n",
    "                arr = []\n",
    "                arr.append(item)\n",
    "                false_prompts[prompt_unit] = arr\n",
    "            #false_prompts[dict_prompt['id']] = \n",
    "                #writer.write(prompt_unit + \"\\t\" + sentence['transcript'] + \"\\n\")\n",
    "        else:\n",
    "            item = {\"id\": dict_prompt['id'], \"prompt\": str(prompt_unit), \"transcript\": transcript, \"processed\": processed, \"unique\": unique_sentence, \"method\": \"RG\", \"language\": True, \"meaning\": True}\n",
    "            safe_prompts.append(item)\n",
    "print(\"Correct: %s\" % str(len(safe_prompts)))\n",
    "print(\"False: %s\" % str(false_counter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "safe_prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def count_false_items():\n",
    "    counter = 0\n",
    "    for key in false_prompts:\n",
    "        counter += len(false_prompts[key])\n",
    "    return counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "count_false_items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(len(safe_prompts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "false_prompts['Sag: Ich möchte mit Dollars bezahlen'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credit Card Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scripts.credit_card as credit_card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def meaning_is_correct(prompt_unit, transcript, clear_transcript, unique_sentence):\n",
    "    nlp_transcript = nlp.nlp_sentence(transcript)\n",
    "    nlp_clear = nlp.nlp_sentence(transcript)\n",
    "    nlp_unqiue = nlp.nlp_sentence(unique_sentence)\n",
    "    extracted_nouns_t = extract_key_nouns(nlp_transcript)\n",
    "    extracted_nouns_c = extract_key_nouns(nlp_clear)\n",
    "    extracted_nouns_u = extract_key_nouns(nlp_unqiue)\n",
    "    \n",
    "    for noun in extracted_nouns_t:\n",
    "        try:\n",
    "            if noun in prompt_noun_map[prompt_unit]:\n",
    "                return True\n",
    "        except:\n",
    "            print(prompt_unit)\n",
    "            print(prompt_noun_map[prompt_unit])\n",
    "            print(noun)\n",
    "    for noun in extracted_nouns_c:\n",
    "        if noun in prompt_noun_map[prompt_unit]:\n",
    "            return True\n",
    "    for noun in extracted_nouns_u:\n",
    "        if noun in prompt_noun_map[prompt_unit]:\n",
    "            return True\n",
    "    return False\n",
    "        #remove_from_false_prompts(false_prompts, key, item['id'], method=\"magic\", meaning=\"correct\", language=\"correct\")\n",
    "    \n",
    "\n",
    "meaning_is_correct(\"Sag: Ich möchte mit Mastercard bezahlen\", \"i want pay with the master card\", \"i want pay with the master card\", \"i want pay with the master card\")\n",
    "#prompt_noun_map[\"Sag: Ich möchte mit Kreditkarte bezahlen\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_prompts = [\"Sag: Ich möchte mit Dollars bezahlen\",\n",
    "\"Sag: Ich möchte mit Euros bezahlen\",\n",
    "\"Sag: Ich möchte mit Kreditkarte bezahlen\",\n",
    "\"Sag: Ich möchte mit Mastercard bezahlen\",\n",
    "\"Sag: Ich möchte mit Postkarte bezahlen\",\n",
    "\"Sag: Ich möchte mit Visa bezahlen\",\n",
    "\"Sag: Ich möchte mit Pfund bezahlen\",\n",
    "\"Sag: Ich möchte mit Schweizer Franken bezahlen\"]\n",
    "\n",
    "false_credit_card_prompts = data_hand.get_test_data_by_false_prompts(test_prompts, false_prompts, grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def remove_from_false_prompts(false_prompts, key, id_, method=\"credit card cluster\", meaning=False, language=False):\n",
    "    if key in false_prompts:\n",
    "        items = false_prompts[key]\n",
    "        for index in range(0, len(items)):\n",
    "            if items[index]['id'] == id_:\n",
    "                item = {\"id\": id_, \n",
    "                        \"prompt\": str(key), \n",
    "                        \"transcript\": items[index]['transcript'], \n",
    "                        \"processed\": items[index][\"processed\"], \n",
    "                        \"method\": method, \"language\": str(language), \n",
    "                        \"meaning\": str(meaning), \n",
    "                        \"unique\": items[index][\"unique\"]}\n",
    "                safe_prompts.append(item)\n",
    "                del false_prompts[key][index]\n",
    "                return item "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162\n",
      "Correct: 0\n",
      "False: 0\n",
      "645\n"
     ]
    }
   ],
   "source": [
    "false_counter = 0\n",
    "correct_counter = 0\n",
    "\n",
    "print(len(false_prompts))\n",
    "\n",
    "credit_card_items = []\n",
    "for prompt_unit in test_prompts:\n",
    "    for item in false_credit_card_prompts[prompt_unit]:\n",
    "        meaning = False\n",
    "        language = False\n",
    "        try:\n",
    "            processed =item[\"processed\"]\n",
    "            unique_sentence = item[\"unique\"]\n",
    "            transcript = item[\"transcript\"]\n",
    "        except:\n",
    "            print(unique_sentence)\n",
    "            continue\n",
    "        if credit_card.accept_credit_card(transcript) == False and \\\n",
    "            credit_card.accept_credit_card(processed) == False and \\\n",
    "            credit_card.accept_credit_card(unique_sentence) == False:\n",
    "            \n",
    "            false_counter += 1\n",
    "            #remove_from_false_prompts(false_prompts, key, item['id'])\n",
    "        else:\n",
    "            #\n",
    "            \n",
    "            language = True\n",
    "        \n",
    "        if meaning_is_correct(prompt_unit, transcript, clear_item, unique_sentence):\n",
    "            meaning = True\n",
    "            \n",
    "        \n",
    "        item = remove_from_false_prompts(false_prompts, prompt_unit, item['id'], meaning=meaning, language=language)\n",
    "        if item:\n",
    "            #print(item)\n",
    "            if item['meaning'] == 'True' and item['language'] == 'True':\n",
    "                correct_counter += 1\n",
    "            credit_card_items.append(item)\n",
    "            \n",
    "print(\"Correct: %s\" % str(correct_counter))\n",
    "print(\"False: %s\" % str(false_counter))\n",
    "print(len(safe_prompts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for item in credit_card_items:\n",
    "    if item['meaning'] == str(True) and item['language']== str(True):\n",
    "        print(item['id'] + \";accepted;\"  + item['language'] + \";\" + item['meaning'])\n",
    "    else:\n",
    "        print(item['id'] + \";rejected;\"  + item['language'] + \";\" + item['meaning'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nouns = []\n",
    "dt_nouns = []\n",
    "for prompt in test_prompts:\n",
    "    for response in grammar[prompt]:\n",
    "        string_it =\" \".join(extract_key_nouns(nlp.nlp_sentence(response)))\n",
    "        if \"ma \" in string_it:\n",
    "            print(response)\n",
    "            print(prompt)\n",
    "        else:\n",
    "            nouns.extend(extract_key_nouns(nlp.nlp_sentence(response)))   \n",
    "            dt_nouns.extend(extract_key_dt_nouns(nlp.nlp_sentence(response)))   \n",
    "\n",
    "generated_nouns = []\n",
    "for noun in list(set(nouns)):\n",
    "    gen = generate_dt_nouns_by_key_nouns(nlp.nlp_sentence(noun))\n",
    "    if len(gen) > 0:\n",
    "        generated_nouns.extend(gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list(set(generated_nouns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for index in range(0, len(correct_items)):\n",
    "    if correct_items[index]['id'] == '3796':\n",
    "        print(correct_items[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([i, wish, to, pay, with, visa],\n",
       " ['i', 'wish', 'to', 'pay', 'with', 'visa'],\n",
       " ['PRP', 'VBP', 'TO', 'VB', 'IN', 'NN'],\n",
       " ['PRP', 'VBP', 'TO', 'VB', 'IN', 'NN'])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.nlp_sentence(\"i wish to pay with visa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "safe_prompts[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restarant Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scripts.restarant as resta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_prompts = [\"Frag: Ich möchte die Rechnung\", \"Frag: Ich möchte die Dessertkarte\"]\n",
    "false_restarant_prompts = data_hand.get_test_data_by_false_prompts(test_prompts, false_prompts, grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162\n",
      "Correct: 0\n",
      "False: 0\n",
      "645\n"
     ]
    }
   ],
   "source": [
    "false_counter = 0\n",
    "correct_counter = 0\n",
    "\n",
    "print(len(false_prompts))\n",
    "for key in test_prompts:\n",
    "    for item in false_restarant_prompts[key]:\n",
    "        try:\n",
    "            processed =item[\"processed\"]\n",
    "            unique_sentence = item[\"unique\"]\n",
    "            transcript = item[\"transcript\"]\n",
    "        except:\n",
    "            continue\n",
    "        if resta.accept_restarant(transcript) == False and \\\n",
    "            resta.accept_restarant(unique_sentence) == False and \\\n",
    "            resta.accept_restarant(processed) == False:\n",
    "            false_counter += 1  \n",
    "            remove_from_false_prompts(false_prompts, key, item['id'], method=\"Restarant Cluster\")\n",
    "        else:\n",
    "            remove_from_false_prompts(false_prompts, key, item['id'], method=\"Restarant Cluster\", meaning=\"correct\", language=\"correct\")\n",
    "            correct_counter += 1\n",
    "print(\"Correct: %s\" % str(correct_counter))\n",
    "print(\"False: %s\" % str(false_counter))\n",
    "print(len(safe_prompts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "false_restarant_prompts = data_hand.get_test_data_by_false_prompts(test_prompts, false_prompts, grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "count_false_items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "safe_prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tickets Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "8\n",
      "here\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['the lion king']"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_by_pattern(patterns, tags, words):\n",
    "    extracted_words = []\n",
    "    tags_string = \" \".join(tags)\n",
    "    for pattern in patterns:\n",
    "        pattern_string = \" \".join(pattern)\n",
    "        if is_slice_in_list(pattern, tags):\n",
    "            pattern_start_index = is_slice_in_list2(pattern, tags)\n",
    "            if len(pattern) > 1:\n",
    "                pattern_end_index = pattern_start_index + len(pattern) -1\n",
    "                print(pattern_start_index)\n",
    "                print(pattern_end_index)\n",
    "                if pattern_end_index - pattern_start_index + 1 == len(pattern):\n",
    "                        extracted_part = \"\"\n",
    "                        print(\"here\")\n",
    "                        for i in range(pattern_start_index, pattern_end_index+1):\n",
    "                            extracted_part += words[i] + \" \"\n",
    "                        extracted_words.append(extracted_part[:-1])\n",
    "            else:\n",
    "                extracted_words.append(words[pattern_start_index])\n",
    "            \"\"\"\n",
    "            if len(pattern) > 1:\n",
    "                pattern_end_index = tags.index(pattern[-1], pattern_start_index+1)\n",
    "                if pattern_end_index - pattern_start_index +1 != len(pattern):\n",
    "                    pattern_end_index = tags.index(pattern[-1], pattern_end_index+1)\n",
    "                    \n",
    "                if pattern_end_index - pattern_start_index + 1 == len(pattern):\n",
    "                    extracted_part = \"\"\n",
    "                    for i in range(pattern_start_index, pattern_end_index+1):\n",
    "                        extracted_part += words[i] + \" \"\n",
    "                        \n",
    "\n",
    "                    extracted_words.append(extracted_part[:-1])\n",
    "            else:\n",
    "                extracted_words.append(words[pattern_start_index])\n",
    "            \"\"\"\n",
    "    return extracted_words\n",
    "def extract_key_dt_nouns(nlp_sent):\n",
    "    tags = nlp_sent[2]\n",
    "    words = nlp.spacy_words_to_string_array(nlp_sent[0])\n",
    "    sentence = \" \".join(words)\n",
    "    #words = \" \".join(nlp_sent[0])\n",
    "    #[['DT', 'NN', 'NN']]\n",
    "    prio_one_patterns = [['DT', 'NN', 'NN'], ['DT', 'NN', 'NNS']]\n",
    "\n",
    "    extracted_words = extract_by_pattern(prio_one_patterns, tags, words)\n",
    "    return extracted_words\n",
    "\n",
    "extract_key_dt_nouns(nlp_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([can, i, buy, three, tickets, for, the, lion, king], ['can', 'i', 'buy', 'three', 'ticket', 'for', 'the', 'lion', 'king'], ['MD', 'PRP', 'VB', 'CD', 'NNS', 'IN', 'DT', 'NN', 'NN'], ['MD', 'PRP', 'VB', 'CD', 'NNS', 'IN', 'DT', 'NN', 'NN'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_s = nlp.nlp_sentence(grammar['Frag: 3 Tickets für König der Löwen'][0])\n",
    "print(nlp_s)\n",
    "extract_key_dt_nouns(nlp_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def insert(tree, key, value):\n",
    "    #print(key)\n",
    "    if key:\n",
    "        first, rest = key[0], key[1:]\n",
    "        if first not in tree:\n",
    "            tree[first] = {}\n",
    "        insert(tree[first], rest, value)\n",
    "    else:\n",
    "        tree['key'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tree = {}\n",
    "for prompt_unit in grammar:\n",
    "    \n",
    "    for response in grammar[prompt_unit]:\n",
    "        tags = nlp.nlp_sentence(response)[2]\n",
    "        insert(tree, tags, \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def existintree(tree, array, rest_tree):\n",
    "    if len(array) == 0:\n",
    "        return False\n",
    "    if array[0] not in rest_tree:\n",
    "        return False\n",
    "    if array[0] in rest_tree:\n",
    "        if 'key' in rest_tree[array[0]] and len(array) == 1:\n",
    "            return True\n",
    "    #print(tree[array[0]])\n",
    "    \n",
    "    return existintree(tree, array[1:], rest_tree[array[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sing_plu(nlp_sent):\n",
    "    words = nlp_sent[0]\n",
    "    tags = nlp_sent[2]\n",
    "    if 'CD' in tags:\n",
    "        index_cd = tags.index('CD')\n",
    "        if str(words[index_cd]) == 'one':\n",
    "            if 'NNS' not in tags:\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "        else:\n",
    "            if len(words) > index_cd + 1 and str(tags[index_cd+1]) == 'NNS':\n",
    "                return True\n",
    "            \n",
    "            elif 'NN' not in tags:\n",
    "                return True\n",
    "            else:\n",
    "                return False \n",
    "    return True\n",
    "\n",
    "def iter_items(prompts, prompt_unit):\n",
    "    magic_accepted_prompts = []\n",
    "    counter = 0\n",
    "    for prompt in prompts:\n",
    "        processed = prompt[\"processed\"]\n",
    "        transcript = prompt[\"transcript\"]\n",
    "        id_ = prompt[\"id\"]\n",
    "        nlp_processed = nlp.nlp_sentence(processed)\n",
    "        nlp_processed_tags = nlp_processed[2]\n",
    "        nlp_transcript = nlp.nlp_sentence(transcript)\n",
    "        nlp_transcript_tags = nlp_transcript[2]\n",
    "        if existintree(tree, nlp_processed_tags, copy.deepcopy(tree)) == True or \\\n",
    "        existintree(tree, nlp_transcript_tags, copy.deepcopy(tree)) == True:\n",
    "            if sing_plu(nlp_processed) == True or sing_plu(nlp_transcript) == True:\n",
    "                item = {\"id\": id_ , \"transcript\": transcript, \"processed\": processed}\n",
    "                \n",
    "                magic_accepted_prompts.append(item)\n",
    "                extracted_nouns = extract_key_nouns(nlp_processed)\n",
    "                for noun in extracted_nouns:\n",
    "                    if noun in prompt_noun_map[prompt_unit]:\n",
    "                        print(item)\n",
    "                        counter += 1\n",
    "                        remove_from_false_prompts(false_prompts, key, item['id'], method=\"magic\", meaning=\"correct\", language=\"correct\")\n",
    "                        \n",
    "                \n",
    "    return magic_accepted_prompts, counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "correct_counter = 0\n",
    "magic_accepted_prompts_map = {}\n",
    "for key in false_prompts:\n",
    "    accepted_prompts, counter = iter_items(false_prompts[key], key)\n",
    "    magic_accepted_prompts_map[key] = accepted_prompts\n",
    "    correct_counter += counter\n",
    "print(correct_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(len(safe_prompts))\n",
    "safe_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"annotated_kaldi_data_v1.csv\", \"w\") as writer:\n",
    "    for item in safe_prompts:\n",
    "        writer.write(item['id'] + \"\\t\" + item['method'] + \"\\t\" + item[\"prompt\"] + \"\\t\" +item['processed'] + \"\\t\" +item['language']+ \"\\t\" + item['meaning'] + \"\\n\" )\n",
    "    for key in false_prompts:\n",
    "        for item in false_prompts[key]:\n",
    "            writer.write(item['id'] + \"\\t\" + \"no method\" + \"\\t\"+ key + \"\\t\" + item['processed'] + \"\\t\" +'incorrect'+ \"\\t\" + 'incorrect' + \"\\n\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prompt_noun_map['Frag: 1 Musical-Ticket']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t_prompt = magic_accepted_prompts_map['Frag: 1 Musical-Ticket'][0][\"transcript\"]\n",
    "print(t_prompt)\n",
    "nlp_t_prompt = nlp.nlp_sentence(t_prompt)\n",
    "print(nlp_t_prompt)\n",
    "extract_key_nouns(nlp_t_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(false_prompts['Frag: 1 Musical-Ticket'])\n",
    "iter_items(false_prompts['Frag: 1 Musical-Ticket'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
