{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading spacy ...\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "import scripts.utils.nlp_utils as nlp\n",
    "import scripts.utils.grammar as gra\n",
    "import scripts.utils.string_handling as string_hand\n",
    "import scripts.utils.data_handler as data_hand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#test_data = data_hand.read_test_data(\"textProcessing_testKaldi.csv\")\n",
    "test_data = data_hand.read_test_data(\"kaldi_test_data_real_v1.tsv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Reference Grammar and Diff Grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reference_grammar = gra.read_grammar_and_create_map('referenceGrammar.xml')\n",
    "diff_grammar = gra.read_grammar_and_create_map('diff_rg_1.xml')\n",
    "grammar = gra.merge_grammars(reference_grammar, diff_grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i will still have a ticket for billy elliot'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string_hand.clear_sentence(\"no i will still have a ticket for billy elliot please\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meaning Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def is_slice_in_list(s,l):\n",
    "    len_s = len(s) #so we don't recompute length of s on every iteration\n",
    "    return any(s == l[i:len_s+i] for i in range(len(l) - len_s+1))\n",
    "\n",
    "def is_slice_in_list2(s,l):\n",
    "    len_s = len(s) #so we don't recompute length of s on every iteration\n",
    "    for i in range(len(l) - len_s+1):\n",
    "        if s==l[i:len_s+i]:\n",
    "            return i\n",
    "        \n",
    "def extract_by_pattern(patterns, tags, words):\n",
    "    extracted_words = []\n",
    "    tags_string = \" \".join(tags)\n",
    "    for pattern in patterns:\n",
    "        pattern_string = \" \".join(pattern)\n",
    "        if is_slice_in_list(pattern, tags):\n",
    "            pattern_start_index = is_slice_in_list2(pattern, tags)\n",
    "            if len(pattern) > 1:\n",
    "                #pattern_end_index = tags.index(pattern[-1], pattern_start_index+1)\n",
    "                pattern_end_index = pattern_start_index + len(pattern) -1\n",
    "                if pattern_end_index - pattern_start_index + 1 == len(pattern):\n",
    "                        extracted_part = \"\"\n",
    "                        for i in range(pattern_start_index, pattern_end_index+1):\n",
    "                            extracted_part += words[i] + \" \"\n",
    "                        extracted_words.append(extracted_part[:-1])\n",
    "            else:\n",
    "                extracted_words.append(words[pattern_start_index])\n",
    "            \"\"\"\n",
    "            if len(pattern) > 1:\n",
    "                pattern_end_index = tags.index(pattern[-1], pattern_start_index+1)\n",
    "                if pattern_end_index - pattern_start_index +1 != len(pattern):\n",
    "                    pattern_end_index = tags.index(pattern[-1], pattern_end_index+1)\n",
    "                    \n",
    "                if pattern_end_index - pattern_start_index + 1 == len(pattern):\n",
    "                    extracted_part = \"\"\n",
    "                    for i in range(pattern_start_index, pattern_end_index+1):\n",
    "                        extracted_part += words[i] + \" \"\n",
    "                        \n",
    "\n",
    "                    extracted_words.append(extracted_part[:-1])\n",
    "            else:\n",
    "                extracted_words.append(words[pattern_start_index])\n",
    "            \"\"\"\n",
    "    return extracted_words\n",
    "    \n",
    "\n",
    "def extract_key_dt_nouns(nlp_sent):\n",
    "    tags = nlp_sent[2]\n",
    "    words = nlp.spacy_words_to_string_array(nlp_sent[0])\n",
    "    sentence = \" \".join(words)\n",
    "    #words = \" \".join(nlp_sent[0])\n",
    "    #[['DT', 'NN', 'NN']]\n",
    "    prio_one_patterns = [['DT', 'NN', 'NN'], ['DT', 'NN', 'NNS'],\n",
    "                         ['DT', 'NN']]\n",
    "\n",
    "    extracted_words = extract_by_pattern(prio_one_patterns, tags, words)\n",
    "    return extracted_words\n",
    "    \n",
    "def extract_key_nouns(nlp_sent):\n",
    "    tags = nlp_sent[2]\n",
    "    words = nlp.spacy_words_to_string_array(nlp_sent[0])\n",
    "    sentence = \" \".join(words)\n",
    "    #words = \" \".join(nlp_sent[0])\n",
    "    #[['DT', 'NN', 'NN']]\n",
    "    prio_one_patterns = [['PRP$', 'NN', 'NN'], ['PRP$', 'NN', 'NNS'], ['PRP$', 'NNS', 'NNS'],\\\n",
    "                         ['PRP$', 'NN'], ['PRP$', 'NNS'], ['NN', 'NNS'], ['NN', 'NN'], ['RB', 'NN'], ['JJ', 'NNS'],\\\n",
    "                         ['JJ', 'NN'], ['NN'], ['NNS']]\n",
    "    \n",
    "    # Only when no prio_one_pattern fits\n",
    "    prio_sec_patterns = [['NN'], ['NNS']]\n",
    "    extracted_words = extract_by_pattern(prio_one_patterns, tags, words)\n",
    "    if len(extracted_words) < 1:\n",
    "        extracted_words = extract_by_pattern(prio_sec_patterns, tags, words)\n",
    "    return extracted_words\n",
    "\n",
    "\n",
    "def generate_dt_nouns_by_key_nouns(nlp_nouns):\n",
    "    #FIX ME a credit cards does work\n",
    "    dts = ['a', 'the']\n",
    "    patterns = [['NN', 'NN'], ['NN', 'NNS'], ['NN']]\n",
    "    tags = nlp_nouns[2]\n",
    "    words = nlp.spacy_words_to_string_array(nlp_nouns[0])\n",
    "    \n",
    "    generated_words = []\n",
    "    for pattern in patterns:\n",
    "        if pattern == tags:\n",
    "            for dt in dts:\n",
    "                sentence = dt + \" \" +\" \".join(words)\n",
    "                generated_words.append(sentence)\n",
    "    return generated_words\n",
    "\n",
    "def generalise_aux_verb(nlp_sent):\n",
    "    tags = nlp_sent[2]\n",
    "    words = nlp.spacy_words_to_string_array(nlp_sent[0])\n",
    "    sentence = \" \".join(words)\n",
    "    patterns = [['PRP', 'MD', 'VB', 'TO'], ['PRP', 'MD', 'VB', 'DT'], ['PRP', 'VBP', 'TO']]\n",
    "\n",
    "    extracted_words = extract_by_pattern(patterns, tags, words)\n",
    "    return extracted_words \n",
    "    \n",
    "def create_meaning_map(grammar):\n",
    "    prompt_noun_map = {}\n",
    "    for prompt in grammar:\n",
    "        try:\n",
    "            extracted_nouns = []\n",
    "            for response in grammar[prompt]:\n",
    "                nlp_s = nlp.nlp_sentence(response)\n",
    "                nouns = extract_key_nouns(nlp_s)\n",
    "                if len(nouns) > 0:\n",
    "                    extracted_nouns.extend(nouns)\n",
    "            prompt_noun_map[prompt] = list(set(extracted_nouns))\n",
    "        except:\n",
    "            print(\"error\")\n",
    "            print(prompt)\n",
    "    return prompt_noun_map\n",
    "\n",
    "prompt_noun_map = create_meaning_map(grammar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Aux verbs from total reference grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I wish to']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def aux_extractor():\n",
    "    found_aux_verb = []\n",
    "    for key in grammar:\n",
    "        for item in grammar[key]:\n",
    "            nlp_s = nlp.nlp_sentence(item)\n",
    "            aux_verb = generalise_aux_verb(nlp_s)\n",
    "            if len(aux_verb) > 0:\n",
    "                found_aux_verb.extend(aux_verb)\n",
    "    print(len(set(found_aux_verb)))\n",
    "    list(set(found_aux_verb))\n",
    "generalise_aux_verb(nlp.nlp_sentence(\"I wish to pay by card\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_nouns_total(prompts, grammar):\n",
    "    nouns = []\n",
    "    dt_nouns = []\n",
    "    for prompt in prompts:\n",
    "        for response in grammar[prompt]:\n",
    "            string_it =\" \".join(extract_key_nouns(nlp.nlp_sentence(response)))\n",
    "            if \"ma \" in string_it:\n",
    "                print(response)\n",
    "                print(prompt)\n",
    "            else:\n",
    "                nouns.extend(extract_key_nouns(nlp.nlp_sentence(response)))   \n",
    "                dt_nouns.extend(extract_key_dt_nouns(nlp.nlp_sentence(response)))   \n",
    "\n",
    "    generated_nouns = []\n",
    "    for noun in list(set(nouns)):\n",
    "        gen = generate_dt_nouns_by_key_nouns(nlp.nlp_sentence(noun))\n",
    "        if len(gen) > 0:\n",
    "            generated_nouns.extend(gen)\n",
    "\n",
    "    return list(set(nouns)), list(set(dt_nouns)), list(set(generated_nouns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_transcript_processed_unique(item):\n",
    "    transcript = item[\"transcript\"]\n",
    "    processed =item[\"processed\"]\n",
    "    unique_sentence = item[\"unique\"]\n",
    "            \n",
    "    return transcript, processed, unique_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply Reference Grammar And Preprocessing And Unqiue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: 578\n",
      "False: 418\n"
     ]
    }
   ],
   "source": [
    "false_counter = 0\n",
    "correct_counter = 0\n",
    "\n",
    "false_prompts = {}\n",
    "safe_prompts = []\n",
    "for prompt_unit in test_data:\n",
    "    \n",
    "    for dict_prompt in test_data[prompt_unit]: \n",
    "        transcript = dict_prompt['transcript']\n",
    "        sentence = transcript\n",
    "        if \"***\" in sentence:\n",
    "            sentence = sentence.replace(\"***\", \"\")\n",
    "        \n",
    "        processed = sentence\n",
    "        \n",
    "        \n",
    "        try:\n",
    "            processed = string_hand.clear_sentence(transcript)\n",
    "        except:\n",
    "            print(dict_prompt)\n",
    "\n",
    "\n",
    "        unique_sentence = string_hand.get_unique_sentence(sentence)\n",
    "        if sentence not in grammar[prompt_unit] and  \\\n",
    "            processed not in grammar[prompt_unit] and \\\n",
    "            unique_sentence not in grammar[prompt_unit]:\n",
    "                \n",
    "            item = {\"id\": dict_prompt['id'], \"prompt\": str(prompt_unit),\"transcript\": transcript, \"processed\": processed, \"unique\": unique_sentence}\n",
    "            false_counter += 1\n",
    "            if prompt_unit in false_prompts:\n",
    "                false_prompts[prompt_unit].append(item)\n",
    "            else:\n",
    "                arr = []\n",
    "                arr.append(item)\n",
    "                false_prompts[prompt_unit] = arr\n",
    "            #false_prompts[dict_prompt['id']] = \n",
    "                #writer.write(prompt_unit + \"\\t\" + sentence['transcript'] + \"\\n\")\n",
    "        else:\n",
    "            item = {\"id\": dict_prompt['id'], \"prompt\": str(prompt_unit), \"transcript\": transcript, \"processed\": processed, \"unique\": unique_sentence, \"method\": \"RG\", \"language\": True, \"meaning\": True}\n",
    "            safe_prompts.append(item)\n",
    "print(\"Correct: %s\" % str(len(safe_prompts)))\n",
    "print(\"False: %s\" % str(false_counter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "578"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(safe_prompts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def count_false_items():\n",
    "    counter = 0\n",
    "    for key in false_prompts:\n",
    "        counter += len(false_prompts[key])\n",
    "    return counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "418"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_false_items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "578\n"
     ]
    }
   ],
   "source": [
    "print(len(safe_prompts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credit Card Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scripts.credit_card as credit_card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def meaning_is_correct(prompt_unit, transcript, clear_transcript, unique_sentence):\n",
    "    nlp_transcript = nlp.nlp_sentence(transcript)\n",
    "    nlp_clear = nlp.nlp_sentence(transcript)\n",
    "    nlp_unqiue = nlp.nlp_sentence(unique_sentence)\n",
    "    extracted_nouns_t = extract_key_nouns(nlp_transcript)\n",
    "    extracted_nouns_c = extract_key_nouns(nlp_clear)\n",
    "    extracted_nouns_u = extract_key_nouns(nlp_unqiue)\n",
    "    \n",
    "    for noun in extracted_nouns_t:\n",
    "        try:\n",
    "            if noun in prompt_noun_map[prompt_unit]:\n",
    "                return True\n",
    "        except:\n",
    "            print(prompt_unit)\n",
    "            print(prompt_noun_map[prompt_unit])\n",
    "            print(noun)\n",
    "    for noun in extracted_nouns_c:\n",
    "        if noun in prompt_noun_map[prompt_unit]:\n",
    "            return True\n",
    "    for noun in extracted_nouns_u:\n",
    "        if noun in prompt_noun_map[prompt_unit]:\n",
    "            return True\n",
    "    return False\n",
    "        #remove_from_false_prompts(false_prompts, key, item['id'], method=\"magic\", meaning=\"correct\", language=\"correct\")\n",
    "    \n",
    "\n",
    "meaning_is_correct(\"Sag: Ich möchte mit Mastercard bezahlen\", \"i want pay with the master card\", \"i want pay with the master card\", \"i want pay with the master card\")\n",
    "#prompt_noun_map[\"Sag: Ich möchte mit Kreditkarte bezahlen\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_prompts = [\"Sag: Ich möchte mit Dollars bezahlen\",\n",
    "\"Sag: Ich möchte mit Euros bezahlen\",\n",
    "\"Sag: Ich möchte mit Kreditkarte bezahlen\",\n",
    "\"Sag: Ich möchte mit Mastercard bezahlen\",\n",
    "\"Sag: Ich möchte mit Postkarte bezahlen\",\n",
    "\"Sag: Ich möchte mit Visa bezahlen\",\n",
    "\"Sag: Ich möchte mit Pfund bezahlen\",\n",
    "\"Sag: Ich möchte mit Schweizer Franken bezahlen\"]\n",
    "\n",
    "false_credit_card_prompts = data_hand.get_test_data_by_false_prompts(test_prompts, false_prompts, grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def remove_from_false_prompts(false_prompts, key, id_, method=\"credit card cluster\", meaning=False, language=False, debug=False):\n",
    "    if key in false_prompts:\n",
    "        items = false_prompts[key]\n",
    "        for index in range(0, len(items)):\n",
    "            if items[index]['id'] == id_:\n",
    "                item = {\"id\": id_, \n",
    "                        \"prompt\": str(key), \n",
    "                        \"transcript\": items[index]['transcript'], \n",
    "                        \"processed\": items[index][\"processed\"], \n",
    "                        \"method\": method, \"language\": str(language), \n",
    "                        \"meaning\": str(meaning), \n",
    "                        \"unique\": items[index][\"unique\"]}\n",
    "\n",
    "                if debug == False:\n",
    "                    safe_prompts.append(item)\n",
    "                    del false_prompts[key][index]\n",
    "                return item "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162\n",
      "Correct: 5\n",
      "False: 44\n",
      "633\n"
     ]
    }
   ],
   "source": [
    "false_counter = 0\n",
    "correct_counter = 0\n",
    "\n",
    "print(len(false_prompts))\n",
    "\n",
    "credit_card_items = []\n",
    "for prompt_unit in test_prompts:\n",
    "    for item in false_credit_card_prompts[prompt_unit]:\n",
    "        meaning = False\n",
    "        language = False\n",
    "        try:\n",
    "            processed =item[\"processed\"]\n",
    "            unique_sentence = item[\"unique\"]\n",
    "            transcript = item[\"transcript\"]\n",
    "        except:\n",
    "            print(unique_sentence)\n",
    "            continue\n",
    "        if credit_card.accept_credit_card(transcript) == False and \\\n",
    "            credit_card.accept_credit_card(processed) == False and \\\n",
    "            credit_card.accept_credit_card(unique_sentence) == False:\n",
    "            \n",
    "            false_counter += 1\n",
    "            #remove_from_false_prompts(false_prompts, key, item['id'])\n",
    "        else:\n",
    "            #\n",
    "            \n",
    "            language = True\n",
    "        \n",
    "        if meaning_is_correct(prompt_unit, transcript, processed, unique_sentence):\n",
    "            meaning = True\n",
    "            \n",
    "        \n",
    "        item = remove_from_false_prompts(false_prompts, prompt_unit, item['id'], meaning=meaning, language=language)\n",
    "        if item:\n",
    "            #print(item)\n",
    "            if item['meaning'] == 'True' and item['language'] == 'True':\n",
    "                correct_counter += 1\n",
    "            credit_card_items.append(item)\n",
    "            \n",
    "print(\"Correct: %s\" % str(correct_counter))\n",
    "print(\"False: %s\" % str(false_counter))\n",
    "print(len(safe_prompts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def not_today():\n",
    "    for item in credit_card_items:\n",
    "        if item['meaning'] == str(True) and item['language']== str(True):\n",
    "            print(item['id'] + \";accepted;\"  + item['language'] + \";\" + item['meaning'])\n",
    "        else:\n",
    "            print(item['id'] + \";rejected;\"  + item['language'] + \";\" + item['meaning'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restarant Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scripts.restarant as resta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_prompts = [\"Frag: Ich möchte die Rechnung\", \"Frag: Ich möchte die Dessertkarte\"]\n",
    "false_restarant_prompts = data_hand.get_test_data_by_false_prompts(test_prompts, false_prompts, grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162\n",
      "Correct: 0\n",
      "False: 13\n",
      "646\n"
     ]
    }
   ],
   "source": [
    "false_counter = 0\n",
    "correct_counter = 0\n",
    "\n",
    "print(len(false_prompts))\n",
    "for key in test_prompts:\n",
    "    for item in false_restarant_prompts[key]:\n",
    "        try:\n",
    "            processed =item[\"processed\"]\n",
    "            unique_sentence = item[\"unique\"]\n",
    "            transcript = item[\"transcript\"]\n",
    "        except:\n",
    "            continue\n",
    "        if resta.accept_restarant(transcript) == False and \\\n",
    "            resta.accept_restarant(unique_sentence) == False and \\\n",
    "            resta.accept_restarant(processed) == False:\n",
    "            false_counter += 1  \n",
    "            remove_from_false_prompts(false_prompts, key, item['id'], method=\"Restarant Cluster\")\n",
    "        else:\n",
    "            remove_from_false_prompts(false_prompts, key, item['id'], method=\"Restarant Cluster\", meaning=\"correct\", language=\"correct\")\n",
    "            correct_counter += 1\n",
    "print(\"Correct: %s\" % str(correct_counter))\n",
    "print(\"False: %s\" % str(false_counter))\n",
    "print(len(safe_prompts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "350"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_false_items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "646"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(safe_prompts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tickets Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ticket_key_nouns = ['ticket', 'tickets']\n",
    "\n",
    "def accept_ticket_meaning(key_nouns_of_prompts, item):\n",
    "    transcript, processed, unique = get_transcript_processed_unique(item)\n",
    "\n",
    "    nlp_t = nlp.nlp_sentence(transcript)\n",
    "    nlp_p = nlp.nlp_sentence(processed)\n",
    "    nlp_u = nlp.nlp_sentence(unique)\n",
    "    \n",
    "    ticket_state = False\n",
    "    key_noun_state = False\n",
    "    for ticket_key_noun in ticket_key_nouns:\n",
    "        if ticket_key_noun in transcript or ticket_key_noun in processed or ticket_key_noun in unique:\n",
    "            ticket_state = True\n",
    "            break\n",
    "    for key_noun in key_nouns_of_prompts:\n",
    "        if key_noun in transcript or key_noun in processed or key_noun in unique:\n",
    "            key_noun_state = True\n",
    "            break\n",
    "    \n",
    "    return ticket_state & key_noun_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accept_ticket_prompts(false_ticket_prompts, ticket_prompts, key_nouns, debug=False):\n",
    "    for prompt in false_ticket_prompts:\n",
    "        if prompt not in ticket_prompts:\n",
    "            print(\"this prompt: %s : is not inside the ticket prompts\" % prompt)\n",
    "        for item in false_ticket_prompts[prompt]:\n",
    "            if debug:\n",
    "                print(str(accept_ticket_meaning(key_nouns, item)) + \"\\t\" + item['processed'])\n",
    "            else:\n",
    "                print(accept_ticket_meaning(key_nouns, item))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### King Of Lions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "king_of_lions_prompts = [\n",
    "\"Frag: 2 Tickets für König der Löwen\",\n",
    "\"Frag trotzdem: ein Ticket für König der Löwen\",\n",
    "\"Frag: 3 Tickets für König der Löwen\",\n",
    "\"Frag: 4 Tickets für König der Löwen\",\n",
    "\"Frag: ein Ticket für König der Löwen\"]\n",
    "\n",
    "king_of_lions_key_nouns = ['the lion king', 'lion king']\n",
    "false_king_of_lions_prompts = data_hand.get_test_data_by_false_prompts(king_of_lions_prompts, false_prompts, grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "accept_ticket_prompts(false_king_of_lions_prompts, king_of_lions_prompts, king_of_lions_key_nouns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mamma mia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "mamma_mia_prompts = [\n",
    "\"Frag: 4 Tickets für Mamma Mia\",\n",
    "\"Frag: ein Ticket für Mamma Mia\",\n",
    "\"Frag trotzdem: ein Ticket für Mamma Mia\",\n",
    "\"Frag: 2 Tickets für Mamma Mia\",\n",
    "\"Frag: 3 Tickets für Mamma Mia\"]\n",
    "\n",
    "mamma_mia_key_nouns = ['mamma mia']\n",
    "false_mamma_mia_prompts = data_hand.get_test_data_by_false_prompts(mamma_mia_prompts, false_prompts, grammar)\n",
    "\n",
    "accept_ticket_prompts(false_mamma_mia_prompts, mamma_mia_prompts, mamma_mia_key_nouns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notting Hill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "notting_hill_prompts = [\"Frag: ein Ticket nach Notting Hill\"]\n",
    "notting_hill_key_nouns = [\"notting hill\", \"nothing hill\"]\n",
    "false_notting_hill_prompts = data_hand.get_test_data_by_false_prompts(notting_hill_prompts, false_prompts, grammar)\n",
    "\n",
    "accept_ticket_prompts(false_notting_hill_prompts, notting_hill_prompts, notting_hill_key_nouns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### National Gallery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "national_gallery_prompts = [\"Frag: Tickets für die National Gallery\"]\n",
    "notting_hill_key_nouns = ['national gallery', 'gallery']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['ticket', 'tickets', 'national gallery', 'gallery', 'i'],\n",
       " ['a ticket'],\n",
       " ['a ticket', 'a gallery', 'the ticket', 'the gallery'])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_nouns_total(national_gallery_prompts, grammar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Places Green Park / Trafalgar / Picca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\tcan i have a ticket to the green park\n",
      "True\tare i would like a ticket to piccadilly circus\n",
      "True\ta ticket from piccadilly circus\n",
      "True\tlike a ticket to piccadilly circus\n",
      "False\ttickets to the have bill three tickets\n",
      "False\ti want a ticket to pick is okay tickets\n",
      "True\ti want a ticket to piccadilly circus tickets\n",
      "True\tcan i have a ticket to the trafalgar square\n"
     ]
    }
   ],
   "source": [
    "place_prompts = [\"Frag: ein Ticket zum Green Park\", \n",
    "                 \"Frag: ein Ticket zum Piccadilly Circus\", \n",
    "                 \"Frag: ein Ticket zum Trafalgar Square\"]\n",
    "place_key_nouns = ['trafalgar square', 'green park', 'piccadilly circus']\n",
    "false_place_prompts = data_hand.get_test_data_by_false_prompts(place_prompts, false_prompts, grammar)\n",
    "accept_ticket_prompts(false_place_prompts, place_prompts, place_key_nouns, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '4308',\n",
       " 'processed': 'tickets to the have bill three tickets',\n",
       " 'prompt': 'Frag: ein Ticket zum Piccadilly Circus',\n",
       " 'transcript': 'tickets to the please have bill three tickets',\n",
       " 'unique': 'tickets to the please have bill three'}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "false_place_prompts['Frag: ein Ticket zum Piccadilly Circus'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([can, i, have, a, ticket, to, trafalgar, square],\n",
       " ['can', 'i', 'have', 'a', 'ticket', 'to', 'trafalgar', 'square'],\n",
       " ['MD', 'PRP', 'VB', 'DT', 'NN', 'IN', 'VB', 'JJ'],\n",
       " ['MD', 'PRP', 'VB', 'DT', 'NN', 'IN', 'VB', 'JJ'])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.nlp_sentence(\"can i have a ticket to trafalgar square\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['ticket'], ['a ticket'], ['a ticket', 'the ticket'])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_nouns_total(notting_hill_prompts, grammar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abend Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "montag_prompts = [\n",
    "    \"Frag: 2 Tickets für Montagabend\",\n",
    "    \"Frag: 3 Tickets für Montagabend\",\n",
    "    \"Frag: 4 Tickets für Montagabend\",\n",
    "    \"Frag: ein Ticket für Montagabend\",\n",
    "    \"Frag: Tickets für Montagabend\"]\n",
    "montag_key_nouns = ['monday', 'monday night', 'monday evening']\n",
    "\n",
    "dienstag_prompts = [\n",
    "    \"Frag: ein Ticket für Dienstagabend\", \n",
    "    \"Frag: Tickets für Dienstagabend\",\n",
    "    \"Frag: 2 Tickets für Dienstagabend\", \n",
    "    \"Frag: 3 Tickets für Dienstagabend\", \n",
    "    \"Frag: 4 Tickets für Dienstagabend\"]\n",
    "dienstag_key_nouns = ['tuesday', 'tuesday evening', 'tuesday night']\n",
    "\n",
    "mittwoch_prompts = [\n",
    "    \"Frag: 2 Tickets für Mittwochabend\",\n",
    "    \"Frag: 3 Tickets für Mittwochabend\",\n",
    "    \"Frag: 4 Tickets für Mittwochabend\",\n",
    "    \"Frag: ein Ticket für Mittwochabend\",\n",
    "    \"Frag: Tickets für Mittwochabend\"]\n",
    "mittwoch_key_nouns = ['wednesday evening', 'wednesday night', 'wednesday']\n",
    "\n",
    "donnerstag_prompts = [\n",
    "    \"Frag: 2 Tickets für Donnerstagabend\",\n",
    "    \"Frag: 3 Tickets für Donnerstagabend\",\n",
    "    \"Frag: 4 Tickets für Donnerstagabend\",\n",
    "    \"Frag: ein Ticket für Donnerstagabend\",\n",
    "    \"Frag: Tickets für Donnerstagabend\"]\n",
    "donnerstag_key_nouns = ['thursday', 'thursday evening', 'thursday night']\n",
    "\n",
    "freitag_prompts =[\n",
    "    \"Frag: 2 Tickets für Freitagabend\",\n",
    "    \"Frag: 3 Tickets für Freitagabend\",\n",
    "    \"Frag: 4 Tickets für Freitagabend\",\n",
    "    \"Frag: ein Ticket für Freitagabend\",\n",
    "    \"Frag: Tickets für Freitagabend\"]\n",
    "freitag_key_nouns = ['friday', 'friday night', 'friday evening']\n",
    "\n",
    "samstag_prompts = [\n",
    "    \"Frag: 2 Tickets für Samstagabend\",\n",
    "    \"Frag: 3 Tickets für Samstagabend\",\n",
    "    \"Frag: 4 Tickets für Samstagabend\",\n",
    "    \"Frag: ein Ticket für Samstagabend\",\n",
    "    \"Frag: Tickets für Samstagabend\"]\n",
    "samstag_key_nouns = ['saturday', 'saturday night', 'saturday evening']\n",
    "\n",
    "sonntag_prompts = [\n",
    "    \"Frag: 2 Tickets für Sonntagabend\",\n",
    "    \"Frag: 3 Tickets für Sonntagabend\",\n",
    "    \"Frag: 4 Tickets für Sonntagabend\",\n",
    "    \"Frag: ein Ticket für Sonntagabend\",\n",
    "    \"Frag: Tickets für Sonntagabend\"]\n",
    "sonntag_key_nouns = ['sunday', 'sunday night', 'sunday evening']\n",
    "\n",
    "heuteabend_prompts = [\n",
    "    \"Frag: 2 Tickets für heute Abend\",\n",
    "    \"Frag: 3 Tickets für heute Abend\",\n",
    "    \"Frag: 4 Tickets für heute Abend\",\n",
    "    \"Frag: ein Ticket für heute Abend\",\n",
    "    \"Frag: Tickets für heute Abend\"]\n",
    "heuteabend_key_nouns = ['tonight', 'evening']\n",
    "\n",
    "morgenabend_prompts = [\n",
    "    \"Frag: 2 Tickets für morgen Abend\",\n",
    "    \"Frag: 3 Tickets für morgen Abend\",\n",
    "    \"Frag: 4 Tickets für morgen Abend\",\n",
    "    \"Frag: ein Ticket für morgen Abend\",\n",
    "    \"Frag: Tickets für morgen Abend\"]\n",
    "morgenabend_key_nouns = ['tomorrow night', 'tomorrow evening', 'tomorrow']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['tomorrow evening',\n",
       "  'tomorrow',\n",
       "  'one ticket',\n",
       "  'tickets',\n",
       "  'tomorrow night',\n",
       "  'one',\n",
       "  'ticket'],\n",
       " ['a ticket', 'the evening'],\n",
       " ['a ticket',\n",
       "  'the tomorrow',\n",
       "  'a tomorrow night',\n",
       "  'the tomorrow evening',\n",
       "  'a tomorrow',\n",
       "  'a tomorrow evening',\n",
       "  'the tomorrow night',\n",
       "  'the ticket'])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_nouns_total(morgenabend_prompts, grammar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Ticket Cluster Pipeline / Meaning Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def prompt_in_ticket_cluster(prompt):\n",
    "    if prompt in mamma_mia_prompts:\n",
    "        return True\n",
    "    elif prompt in king_of_lions_prompts:\n",
    "        return True\n",
    "    elif prompt in notting_hill_prompts:\n",
    "        return True\n",
    "    elif prompt in place_prompts:\n",
    "        return True\n",
    "    elif prompt in montag_prompts:\n",
    "        return True\n",
    "    elif prompt in dienstag_prompts:\n",
    "        return True\n",
    "    elif prompt in mittwoch_prompts:\n",
    "        return True\n",
    "    elif prompt in donnerstag_prompts:\n",
    "        return True\n",
    "    elif prompt in freitag_prompts:\n",
    "        return True\n",
    "    elif prompt in samstag_prompts:\n",
    "        return True\n",
    "    elif prompt in sonntag_prompts:\n",
    "        return True\n",
    "    elif prompt in heuteabend_prompts:\n",
    "        return True\n",
    "    elif prompt in morgenabend_prompts:\n",
    "        return True\n",
    "    elif prompt in national_gallery_prompts:\n",
    "        return True\n",
    "    return False\n",
    "        \n",
    "def ticket_cluster_pipeline(prompt, item):\n",
    "    if prompt in mamma_mia_prompts:\n",
    "        return accept_ticket_meaning(mamma_mia_key_nouns, item)\n",
    "    elif prompt in king_of_lions_prompts:\n",
    "        return accept_ticket_meaning(king_of_lions_key_nouns, item)\n",
    "    elif prompt in notting_hill_prompts:\n",
    "        return accept_ticket_meaning(notting_hill_key_nouns, item)\n",
    "    elif prompt in place_prompts:\n",
    "        return accept_ticket_meaning(place_key_nouns, item)\n",
    "    elif prompt in montag_prompts:\n",
    "        return accept_ticket_meaning(montag_key_nouns, item)\n",
    "    elif prompt in dienstag_prompts:\n",
    "        return accept_ticket_meaning(dienstag_key_nouns, item)\n",
    "    elif prompt in mittwoch_prompts:\n",
    "        return accept_ticket_meaning(mittwoch_key_nouns, item)\n",
    "    elif prompt in donnerstag_prompts:\n",
    "        return accept_ticket_meaning(donnerstag_key_nouns, item)\n",
    "    elif prompt in freitag_prompts:\n",
    "        return accept_ticket_meaning(freitag_key_nouns, item)\n",
    "    elif prompt in samstag_prompts:\n",
    "        return accept_ticket_meaning(samstag_key_nouns, item)\n",
    "    elif prompt in sonntag_prompts:\n",
    "        return accept_ticket_meaning(sonntag_key_nouns, item)\n",
    "    elif prompt in heuteabend_prompts:\n",
    "        return accept_ticket_meaning(heuteabend_key_nouns, item)\n",
    "    elif prompt in morgenabend_prompts:\n",
    "        return accept_ticket_meaning(morgenabend_key_nouns, item)\n",
    "    elif prompt in national_gallery_prompts:\n",
    "        return accept_ticket_meaning(notting_hill_key_nouns, item)\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': 'Frag: ein Ticket für Mamma Mia', 'id': '3838', 'unique': 'i want a ticket for mamma mia', 'processed': 'i want a ticket for mamma mia', 'transcript': 'i want a ticket for mamma mia'}\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "test_prompt = 'Frag: ein Ticket für Mamma Mia'\n",
    "test_item = false_mamma_mia_prompts[test_prompt][0]\n",
    "print(test_item)\n",
    "if prompt_in_ticket_cluster(test_prompt) == True:\n",
    "    meaning = ticket_cluster_pipeline(test_prompt, test_item)\n",
    "    print(meaning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def insert(tree, key, value):\n",
    "    #print(key)\n",
    "    if key:\n",
    "        first, rest = key[0], key[1:]\n",
    "        if first not in tree:\n",
    "            tree[first] = {}\n",
    "        insert(tree[first], rest, value)\n",
    "    else:\n",
    "        tree['key'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tree = {}\n",
    "for prompt_unit in grammar:\n",
    "    \n",
    "    for response in grammar[prompt_unit]:\n",
    "        tags = nlp.nlp_sentence(response)[2]\n",
    "        insert(tree, tags, \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def existintree(tree, array, rest_tree):\n",
    "    if len(array) == 0:\n",
    "        return False\n",
    "    if array[0] not in rest_tree:\n",
    "        return False\n",
    "    if array[0] in rest_tree:\n",
    "        if 'key' in rest_tree[array[0]] and len(array) == 1:\n",
    "            return True\n",
    "    #print(tree[array[0]])\n",
    "    \n",
    "    return existintree(tree, array[1:], rest_tree[array[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sing_plu(nlp_sent):\n",
    "    words = nlp_sent[0]\n",
    "    tags = nlp_sent[2]\n",
    "    if 'CD' in tags:\n",
    "        index_cd = tags.index('CD')\n",
    "        if str(words[index_cd]) == 'one':\n",
    "            if 'NNS' not in tags:\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "        else:\n",
    "            if len(words) > index_cd + 1 and str(tags[index_cd+1]) == 'NNS':\n",
    "                return True\n",
    "            \n",
    "            elif 'NN' not in tags:\n",
    "                return True\n",
    "            else:\n",
    "                return False \n",
    "    return True\n",
    "\n",
    "def iter_items(prompts, prompt_unit, debug=False):\n",
    "    magic_accepted_prompts = []\n",
    "    counter = 0\n",
    "    \n",
    "    for prompt in prompts:\n",
    "        processed = prompt[\"processed\"]\n",
    "        transcript = prompt[\"transcript\"]\n",
    "        id_ = prompt[\"id\"]\n",
    "        nlp_processed = nlp.nlp_sentence(processed)\n",
    "        nlp_processed_tags = nlp_processed[2]\n",
    "        nlp_transcript = nlp.nlp_sentence(transcript)\n",
    "        nlp_transcript_tags = nlp_transcript[2]\n",
    "        \n",
    "        meaning = False\n",
    "        meaning_reject = False\n",
    "        if prompt_in_ticket_cluster(prompt_unit) == True:\n",
    "            meaning = ticket_cluster_pipeline(prompt_unit, prompt)\n",
    "            if meaning == False:\n",
    "                meaning_reject = True\n",
    "            \n",
    "        if existintree(tree, nlp_processed_tags, copy.deepcopy(tree)) == True or \\\n",
    "        existintree(tree, nlp_transcript_tags, copy.deepcopy(tree)) == True:\n",
    "            if sing_plu(nlp_processed) == True or sing_plu(nlp_transcript) == True:\n",
    "                item = {\"id\": id_ , \"transcript\": transcript, \"processed\": processed, \"language\":True, \"meaning\":meaning}  \n",
    "\n",
    "                if meaning_reject == True:\n",
    "                    item = remove_from_false_prompts(false_prompts, prompt_unit, item['id'], method=\"meaning_reject\", meaning=meaning, language=True, debug=debug)\n",
    "                    print(\"meaning reject \\t id: %s\" % str(item['id']))\n",
    "                    continue\n",
    "                else:\n",
    "                    if meaning == True:\n",
    "                        item = remove_from_false_prompts(false_prompts, prompt_unit, item['id'], method=\"magic\", meaning=meaning, language=True, debug=debug)\n",
    "\n",
    "                        counter += 1\n",
    "                    else:\n",
    "                        extracted_nouns = extract_key_nouns(nlp_processed)\n",
    "                        for noun in extracted_nouns:\n",
    "                            if noun in prompt_noun_map[prompt_unit]:\n",
    "                                #print(item)\n",
    "                                counter += 1\n",
    "                                item = remove_from_false_prompts(false_prompts, prompt_unit, item['id'], method=\"magic\", meaning=True, language=True, debug=debug)\n",
    "                                break\n",
    "                magic_accepted_prompts.append(item)\n",
    "                        \n",
    "    return magic_accepted_prompts, counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meaning reject \t id: 3628\n",
      "meaning reject \t id: 4542\n",
      "meaning reject \t id: 4294\n",
      "13\n"
     ]
    }
   ],
   "source": [
    "correct_counter = 0\n",
    "magic_accepted_prompts_map = {}\n",
    "debug = False\n",
    "for key in false_prompts:\n",
    "    \n",
    "    if debug == False:\n",
    "        accepted_prompts, counter = iter_items(false_prompts[key], key)\n",
    "        magic_accepted_prompts_map[key] = accepted_prompts\n",
    "        correct_counter += counter\n",
    "    else:\n",
    "        accepted_prompts, counter = iter_items(false_prompts[key], key, debug)\n",
    "        correct_counter += counter\n",
    "print(correct_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"annotated_kaldi_data_v8.csv\", \"w\") as writer:\n",
    "    for item in safe_prompts:\n",
    "        accepted = False\n",
    "        if str(item['language']) == str(True) and str(item['meaning']) == str(True): \n",
    "            accepted = True\n",
    "        writer.write(item['id'] + \"\\t\" + item['method'] + \"\\t\" + item[\"prompt\"] + \"\\t\" +item['processed'] + \"\\t\" + str(item['language'])+ \"\\t\" + str(item['meaning']) + \"\\t\" + str(accepted)+ \"\\n\" )\n",
    "    for key in false_prompts:\n",
    "        for item in false_prompts[key]:\n",
    "            writer.write(item['id'] + \"\\t\" + \"no method\" + \"\\t\"+ key + \"\\t\" + item['processed'] + \"\\t\" +'FALSE'+ \"\\t\" + 'FALSE' + \"\\t\" + \"FALSE\" + \"\\n\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3605 \n",
      "Different judgement: NOW: True, OLD: FALSE \n",
      "Different Meaning: NOW: True, OLD: FALSE \n",
      "Different Language: NOW: True, OLD: FALSE \n",
      "\n",
      "4532 \n",
      "Different judgement: NOW: False, OLD: True \n",
      "Different Meaning: NOW: False, OLD: True \n",
      "\n",
      "3583 \n",
      "Different judgement: NOW: False, OLD: True \n",
      "Different Meaning: NOW: False, OLD: True \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def read_attributes(item):\n",
    "    split = item.replace(\"\\n\", \"\").split(\"\\t\")\n",
    "    id_ = split[0]\n",
    "    method_ = split[1]\n",
    "    prompt_ = split[2]\n",
    "    transcript_ = split[3]\n",
    "    language_ = split[4]\n",
    "    meaning_ = split[5]\n",
    "    judgement_ = split[6]\n",
    "    return id_, {\n",
    "        \"method\":method_, \n",
    "        \"prompt\":prompt_, \n",
    "        \"transcript\":transcript_, \n",
    "        \"language\": language_, \n",
    "        \"meaning\":meaning_, \n",
    "        \"judgement\":judgement_}\n",
    "\n",
    "def create_id_map_from_generated_data(file):\n",
    "    data = None\n",
    "    data_map = {}\n",
    "    with open(file, \"r\") as reader1:\n",
    "        data = reader1.readlines()\n",
    "        for dat in data:\n",
    "            id_, item = read_attributes(dat)\n",
    "            data_map[id_] = item\n",
    "    return data_map\n",
    "            \n",
    "def compare_results(this_results, last_results):\n",
    "    for id_ in this_results:\n",
    "        if id_ not in last_results:\n",
    "            print(\"ERROR: Why is this id %s not inside the old results?\" % id_)\n",
    "        difference_string = \"%s \\n\" % str(id_)\n",
    "        difference= False\n",
    "        if str(this_results[id_]['judgement']) != str(last_results[id_]['judgement']):\n",
    "            difference_string += \"Different judgement: NOW: %s, OLD: %s \\n\" % (this_results[id_]['judgement'],  str(last_results[id_]['judgement']))\n",
    "            difference = True\n",
    "        if str(this_results[id_]['transcript']) != str(last_results[id_]['transcript']):\n",
    "            difference_string += \"Different Transcripts: \\nNOW: %s \\nOLD: %s \\n\" % (this_results[id_]['transcript'],  str(last_results[id_]['transcript']))\n",
    "            difference = True\n",
    "        \n",
    "        if str(this_results[id_]['meaning']) != str(last_results[id_]['meaning']):\n",
    "            difference_string += \"Different Meaning: NOW: %s, OLD: %s \\n\" % (this_results[id_]['meaning'],  str(last_results[id_]['meaning']))\n",
    "            difference = True\n",
    "            \n",
    "        if str(this_results[id_]['language']) != str(last_results[id_]['language']):\n",
    "            difference_string += \"Different Language: NOW: %s, OLD: %s \\n\" % (this_results[id_]['language'],  str(last_results[id_]['language']))\n",
    "            difference = True\n",
    "            \n",
    "        if difference:\n",
    "            print(difference_string)\n",
    "            \n",
    "this_results = create_id_map_from_generated_data(\"results/annotated_kaldi_data_v8.csv\")\n",
    "old_results = create_id_map_from_generated_data(\"results/annotated_kaldi_data_v7.csv\")\n",
    "compare_results(this_results, old_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
