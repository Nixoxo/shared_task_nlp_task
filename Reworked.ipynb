{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading spacy ...\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "import scripts.utils.nlp_utils as nlp\n",
    "import scripts.utils.grammar as gra\n",
    "import scripts.utils.string_handling as string_hand\n",
    "import scripts.utils.data_handler as data_hand\n",
    "import scripts.utils.dscore as dscore\n",
    "import scripts.utils.meaning as meaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reference_grammar = gra.read_grammar_and_create_map('referenceGrammar.xml')\n",
    "diff_grammar = gra.read_grammar_and_create_map('diff_rg_1.xml')\n",
    "grammar = gra.merge_grammars(reference_grammar, diff_grammar)\n",
    "prompt_noun_map = meaning.create_meaning_map(grammar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#path_to_test_data = \"original_data.csv\"\n",
    "# kaldi\n",
    "path_to_test_data = \"data/textProcessing_testKaldi.csv\"\n",
    "path_to_test_data = \"data/v11.csv\"\n",
    "path_to_test_data = \"data/JJJ_output.csv\"\n",
    "#path_to_test_data = \"data/speechrecAlldata.csv\"\n",
    "#path_to_test_data = \"data/speechRecAllDataDrive.csv\"\n",
    "#nuance\n",
    "#path_to_test_data =\"data/textProcessing_testNuance.csv\"\n",
    "#path_to_test_data = \"data/testDataAnnotated.csv\"\n",
    "speech_rec_data = data_hand.read_test_data(path_to_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#grammar['Frag: 4 Tickets nach London']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NGram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "id_sirlm = {}\n",
    "with open('languagesolv/v5/id_srilm.csv', 'r') as reader:\n",
    "    lines = reader.readlines()\n",
    "    for line in lines:\n",
    "        split = line.replace(\"\\n\", \"\").split(\"\\t\")\n",
    "        id_ = split[0]\n",
    "        logprob = float(split[1])\n",
    "        ppl = float(split[2])\n",
    "        ppl1 = float(split[3])\n",
    "        id_sirlm[id_] = {'logprob': logprob, 'ppl':ppl, 'ppl1':ppl1}    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classified_prompt = {}\n",
    "        \n",
    "def get_judgement(response):\n",
    "    if response['meaning'] == True and response['language'] == True:\n",
    "        response['judgement'] = 'accept'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for prompt_unit in speech_rec_data:\n",
    "    for response in speech_rec_data[prompt_unit]:\n",
    "        id_ = response['id']\n",
    "        #if id_ == '3892' or id_== 'Id' or id_=='3570' or id_ == '3751':\n",
    "        #    continue\n",
    "        if id_ == 'Id':\n",
    "            continue\n",
    "        if id_ == str(4361) or id_ == str(0) or id_==str(0):\n",
    "            continue\n",
    "        #if float(id_sirlm[id_]['ppl1']) <19.3138:\n",
    "        #if float(id_sirlm[id_]['ppl1']) < 20:\n",
    "        #if float(id_sirlm[id_]['logprob']) <= -31593.3008: #4.699\n",
    "        #if float(id_sirlm[id_]['logprob']) <= -31117.5039: #4.699 \n",
    "        #if float(id_sirlm[id_]['logprob']) <= -28392.2188: #4.699     \n",
    "        if float(id_sirlm[id_]['logprob']) <= -29645.3398:\n",
    "        #if float(id_sirlm[id_]['ppl']) <= 33.2845:\n",
    "        \n",
    "            response['language'] = True\n",
    "\n",
    "        #elif float(id_sirlm[id_]['ppl']) < 15.5:\n",
    "        else:\n",
    "            response['language'] = False   \n",
    "        get_judgement(response)\n",
    "        classified_prompt[response['id']] = response['judgement']          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "score_map, this_score = dscore.score_data(classified_prompt)\n",
    "dscore.print_scores(this_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Meaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_id_map():\n",
    "    id_map = {}\n",
    "    for prompt_unit in speech_rec_data:\n",
    "        for response in speech_rec_data[prompt_unit]:\n",
    "            id_map[response['id']] = response\n",
    "            \n",
    "    return id_map\n",
    "\n",
    "id_map = create_id_map()\n",
    "\n",
    "def response_in_grammar(response, grammar_responses):\n",
    "    return response['transcript'] in grammar_responses or response['unique'] in grammar_responses or response['processed'] in grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path_to_reference = \"referencetest_final.csv\"\n",
    "reference_id_map = data_hand.read_reference(path_to_reference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference Grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def apply_reference_grammar(thisgrammar, speech_data):\n",
    "    for prompt_unit in speech_data:\n",
    "        if prompt_unit == 'Prompt':\n",
    "            continue\n",
    "        for response in speech_data[prompt_unit]: \n",
    "\n",
    "            if response_in_grammar(response, thisgrammar[prompt_unit]):\n",
    "                response['meaning'] = True\n",
    "                response['language'] = True\n",
    "            get_judgement(response)\n",
    "            classified_prompt[response['id']] = response['judgement']          \n",
    "apply_reference_grammar(grammar, speech_rec_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "score_map, this_score = dscore.score_data(classified_prompt)\n",
    "dscore.print_scores(this_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "4.778-3.158"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credit Card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scripts.credit_card as credit_card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "credit_card_map = {}\n",
    "def apply_credit_card(speech_data):\n",
    "    credit_card_prompts = credit_card.get_prompts()\n",
    "    for prompt_unit in credit_card_prompts:\n",
    "        for response in speech_data[prompt_unit]:            \n",
    "            if credit_card.accept_credit_card(response['transcript']) or \\\n",
    "                        credit_card.accept_credit_card(response['processed']) or \\\n",
    "                        credit_card.accept_credit_card(response['unique']):  \n",
    "                response['language'] = True\n",
    "                \n",
    "            if meaning.meaning_is_correct(prompt_noun_map, prompt_unit, response['transcript'], response['processed'], response['unique']):\n",
    "                response['meaning'] = True\n",
    "            \n",
    "            get_judgement(response)\n",
    "            classified_prompt[response['id']] = response['judgement']\n",
    "            credit_card_map[response['id']] = response['judgement']\n",
    "\n",
    "apply_credit_card(speech_rec_data)\n",
    "score_map, this_score = dscore.score_data(credit_card_map)\n",
    "dscore.print_scores(this_score)\n",
    "CorrectAccept, GrossFalseAccept, PlainFalseAccept, CorrectReject, FalseReject = dscore.get_single_results(score_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restarant Cluster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scripts.restarant as resta\n",
    "import scripts.utils.meaning as meaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "resta.get_prompts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "restarant_map = {}\n",
    "def apply_restarant(speech_data):\n",
    "    restarant_prompts = resta.get_prompts()\n",
    "    for prompt_unit in restarant_prompts:\n",
    "        for response in speech_data[prompt_unit]:            \n",
    "            if resta.accept_restarant(response['transcript']) or \\\n",
    "                resta.accept_restarant(response['processed']) or \\\n",
    "                resta.accept_restarant(response['unique']):  \n",
    "                response['language'] = True\n",
    "                \n",
    "                \"\"\"\n",
    "            if response['processed'] == \"i would like to pay\":\n",
    "                response['language'] = True\n",
    "                response['meaning'] = True\n",
    "                \"\"\"\n",
    "            \n",
    "            if meaning.meaning_is_correct(prompt_noun_map, prompt_unit, response['transcript'], response['processed'], response['unique']):\n",
    "                response['meaning'] = True\n",
    "            get_judgement(response)\n",
    "            \n",
    "            classified_prompt[response['id']] = response['judgement']\n",
    "            \n",
    "            if classified_prompt[response['id']] != response['judgement']:\n",
    "                print(\"asdf\")\n",
    "            restarant_map[response['id']] = response['judgement']\n",
    "\n",
    "apply_restarant(speech_rec_data)\n",
    "score_map, this_score = dscore.score_data(restarant_map)\n",
    "dscore.print_scores(this_score)\n",
    "CorrectAccept, GrossFalseAccept, PlainFalseAccept, CorrectReject, FalseReject = dscore.get_single_results(score_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_, GrossFalseAccept, PlainFalseAccept, _, FalseReject = dscore.get_single_results(score_map)\n",
    "#analyse_false_classifed_ones(GrossFalseAccept, PlainFalseAccept, FalseReject)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zimmer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scripts.zimmer as zimmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "zimmer_map = {}\n",
    "def apply_zimmer(speech_data):\n",
    "    zimmer_prompts = zimmer.get_prompts()\n",
    "    for prompt_unit in zimmer_prompts:\n",
    "        for response in speech_data[prompt_unit]:            \n",
    "            if zimmer.accept_zimmer(prompt_unit, response):  \n",
    "                response['meaning'] = True\n",
    "            get_judgement(response)\n",
    "            \n",
    "            zimmer_map[response['id']] = response['judgement']\n",
    "\n",
    "apply_zimmer(speech_rec_data)\n",
    "score_map, this_score = dscore.score_data(zimmer_map)\n",
    "dscore.print_scores(this_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Capital"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scripts.capital as capital"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "capital_map = {}\n",
    "def apply_capital(speech_data):\n",
    "    capital_prompts = capital.get_prompts()\n",
    "    for prompt_unit in capital_prompts:\n",
    "        if prompt_unit in speech_data:\n",
    "            for response in speech_data[prompt_unit]:            \n",
    "                if capital.accept_capital(prompt_unit, response):  \n",
    "                    response['meaning'] = True\n",
    "                get_judgement(response)\n",
    "                capital_map[response['id']] = response['judgement']\n",
    "apply_capital(speech_rec_data)\n",
    "score_map, this_score = dscore.score_data(capital_map)\n",
    "dscore.print_scores(this_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ticket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scripts.ticket as ticket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ticket_map = {}\n",
    "def apply_capital(speech_data):\n",
    "    ticket_prompts = ticket.get_prompts()\n",
    "    for prompt_unit in ticket_prompts:\n",
    "        if prompt_unit in speech_data:\n",
    "            for response in speech_data[prompt_unit]:            \n",
    "                if ticket.accept_ticket(prompt_unit, response):  \n",
    "                    response['meaning'] = True\n",
    "                get_judgement(response)\n",
    "                ticket_map [response['id']] = response['judgement']\n",
    "apply_capital(speech_rec_data)\n",
    "score_map, this_score = dscore.score_data(ticket_map )\n",
    "dscore.print_scores(this_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_, GrossFalseAccept, PlainFalseAccept, _, FalseReject = dscore.get_single_results(score_map)\n",
    "analyse_false_classifed_ones(FalseReject=FalseReject, showFalse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prompts_done = []\n",
    "prompts_done.extend(credit_card.get_prompts())\n",
    "prompts_done.extend(resta.get_prompts())\n",
    "prompts_done.extend(zimmer.get_prompts())\n",
    "#prompts_done.extend(ticket.get_prompts())\n",
    "prompts_done.extend(capital.get_prompts())\n",
    "prompts_done.remove('Frag: Ich möchte die Dessertkarte')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "def insert(tree, key, value):\n",
    "    #print(key)\n",
    "    if key:\n",
    "        first, rest = key[0], key[1:]\n",
    "        if first not in tree:\n",
    "            tree[first] = {}\n",
    "        insert(tree[first], rest, value)\n",
    "    else:\n",
    "        tree['key'] = True\n",
    "\n",
    "tree = {}\n",
    "\n",
    "def create_tree():\n",
    "    for prompt_unit in grammar:\n",
    "\n",
    "        for response in grammar[prompt_unit]:\n",
    "            tags = nlp.nlp_sentence(response)[2]\n",
    "            insert(tree, tags, \"true\")\n",
    "def existintree(tree, array, rest_tree):\n",
    "    if len(array) == 0:\n",
    "        return False\n",
    "    if array[0] not in rest_tree:\n",
    "        return False\n",
    "    if array[0] in rest_tree:\n",
    "        if 'key' in rest_tree[array[0]] and len(array) == 1:\n",
    "            return True\n",
    "    #print(tree[array[0]])\n",
    "    \n",
    "    return existintree(tree, array[1:], rest_tree[array[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "false_friends_map = {}\n",
    "false_friends_map = {\"Frag: Ich möchte die Dessertkarte\": ['dessert card']}\n",
    "def false_friends(prompt, sentence):\n",
    "    if prompt in false_friends_map:\n",
    "        for false in false_friends_map[prompt]:\n",
    "            if false in sentence:\n",
    "                return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def apply_some_magic(thisgrammar, speech_data):\n",
    "    for prompt_unit in speech_data:\n",
    "        if prompt_unit == 'Prompt':\n",
    "            continue\n",
    "        if prompt_unit in prompts_done:\n",
    "            continue\n",
    "\n",
    "        for response in speech_data[prompt_unit]: \n",
    "            if meaning.meaning_is_correct(prompt_noun_map, prompt_unit, response['transcript'], response['processed'], response['unique']):\n",
    "                response['meaning'] = True\n",
    "            if false_friends(prompt_unit, response['processed']):\n",
    "                response['meaning'] = False\n",
    "            \n",
    "            get_judgement(response)\n",
    "            \n",
    "            classified_prompt[response['id']] = response['judgement']          \n",
    "apply_some_magic(grammar, speech_rec_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "score_map, this_score = dscore.score_data(classified_prompt)\n",
    "dscore.print_scores(this_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#score_map, this_score = dscore.score_data(classified_prompt)\n",
    "#dscore.print_scores(this_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "meaning_and_language_acceptance_rate(speech_rec_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#stop\n",
    "meaning_and_language_acceptance_rate(speech_rec_data)\n",
    "#reference_id_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_, GrossFalseAccept, PlainFalseAccept, _, FalseReject = dscore.get_single_results(score_map)\n",
    "analyse_false_classifed_ones(GrossFalseAccept=GrossFalseAccept, showGross=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_items(id_,id_entry, reference_entry, typ):\n",
    "    prompt = reference_entry['prompt']\n",
    "    \n",
    "    transcript = data_hand.transcript_processed_unique(reference_entry['transcript'])[0]\n",
    "    recresult = id_entry['processed']\n",
    "    \n",
    "    \n",
    "    language_their = boolean_to_correct_incorrect(reference_entry['language'])\n",
    "    language_our = boolean_to_correct_incorrect(id_entry['language'])\n",
    "    \n",
    "    \n",
    "    meaning_their = boolean_to_correct_incorrect(reference_entry['meaning'])\n",
    "    meaning_our = boolean_to_correct_incorrect(id_entry['meaning'])\n",
    "    \n",
    "    return \"%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\n\" % (id_, prompt, transcript, recresult, language_their, language_our, meaning_their, meaning_our, typ)\n",
    "#ID PROMPT Transcript(Their) RecResult(Our) Language(Their) Language(Our) Meaning(Their) Meaning(Our) Type\n",
    "def write_to_file(GrossFalseAccept, PlainFalseAccept, FalseReject):\n",
    "    with open(\"missclassified_eigenem_wert.csv\", 'w') as writer:\n",
    "        writer.write(\"%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\n\" % (\"Id\", \"Prompt\", \"Transcript (Their)\", \"RecResult (Our)\", \"Language(Their)\", \"Language(Our)\", \"Meaning(Their)\", \"Meaning(Our)\", \"Type\"))\n",
    "        for id_ in GrossFalseAccept:\n",
    "            writer.write(get_items(id_, id_map[id_], reference_id_map[id_], \"GrossFalseAccept\"))\n",
    "\n",
    "        for id_ in PlainFalseAccept:\n",
    "            writer.write(get_items(id_, id_map[id_], reference_id_map[id_], \"PlainFalseAccept\"))\n",
    "\n",
    "        for id_ in FalseReject:\n",
    "            writer.write(get_items(id_, id_map[id_], reference_id_map[id_], \"FalseReject\"))\n",
    "\n",
    "write_to_file(GrossFalseAccept, PlainFalseAccept, FalseReject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "GrossFalseAccept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def boolean_to_correct_incorrect(boolean):\n",
    "    if boolean:\n",
    "        return 'correct'\n",
    "    return 'incorrect'\n",
    "    \n",
    "def print_comparision(id_, id_entry, reference_entry):\n",
    "    print(\"ID: %s Prompt: %s\" % (id_, reference_entry['prompt']))\n",
    "    print(\"TRANSCRIPT\")\n",
    "    print(\"\\tour:      %s\" % id_entry['processed'])\n",
    "    print(\"\\ttheir:    %s\" % data_hand.transcript_processed_unique(reference_entry['transcript'])[0])\n",
    "    print(\"LANGUAGE\")\n",
    "    print(\"\\tour:     %s\" % boolean_to_correct_incorrect(id_entry['language']))\n",
    "    print(\"\\ttheir:   %s\" % boolean_to_correct_incorrect(reference_entry['language']))\n",
    "    print(\"MEANING\")\n",
    "    print(\"\\tour:     %s\" % boolean_to_correct_incorrect(id_entry['meaning']))\n",
    "    print(\"\\ttheir:   %s\" % boolean_to_correct_incorrect(reference_entry['meaning']))\n",
    "    print(\"\\n\")\n",
    "    \n",
    "def analyse_false_classifed_ones(GrossFalseAccept=None, PlainFalseAccept=None, FalseReject=None, showGross=False, showPlain=False, showFalse=False):\n",
    "    if showGross:\n",
    "        for id_ in GrossFalseAccept:\n",
    "            print(\"GrossFalseAccept\")\n",
    "            print_comparision(id_, id_map[id_], reference_id_map[id_])\n",
    "    if showPlain:\n",
    "        for id_ in PlainFalseAccept:\n",
    "            print(\"PlainFalseAccept\")\n",
    "            print_comparision(id_, id_map[id_], reference_id_map[id_])\n",
    "\n",
    "    if showFalse:\n",
    "        for id_ in FalseReject:\n",
    "            print(\"FalseReject\")\n",
    "            print_comparision(id_, id_map[id_], reference_id_map[id_])\n",
    "\n",
    "def meaning_and_language_acceptance_rate(speech_data):\n",
    "    total = len(reference_id_map)\n",
    "    meaning = 0\n",
    "    language = 0\n",
    "    for prompt_unit in speech_data:\n",
    "        for response in speech_data[prompt_unit]:\n",
    "            if response['id'] == 'Id':\n",
    "                continue\n",
    "            if response['meaning'] == reference_id_map[response['id']]['meaning']:\n",
    "                meaning +=1\n",
    "            if response['language'] == reference_id_map[response['id']]['language']:\n",
    "                language +=1\n",
    "    \n",
    "    print(\"Meaning: %s\" % str(meaning/total*100))\n",
    "    print(\"Language: %s\" % str(language/total*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reference_id_map['3801']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "id_map['3801']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "PlainFalseAccept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference For Looking UP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scripts.utils.dscore as dscore\n",
    "score_map, this_score = dscore.score_data(classified_prompt)\n",
    "dscore.print_scores(this_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "language_reference_map = {}\n",
    "meaning_reference_map = {}\n",
    "with open('referencetest_final.csv','r') as reader:\n",
    "    lines = reader.readlines()\n",
    "    for line in lines[1:]:\n",
    "        split = line.replace(\"\\n\", \"\").split(\"\\t\")\n",
    "        id_ = split[0]\n",
    "        language = split[4]\n",
    "        meaning = split[5]\n",
    "        \n",
    "        if language == 'incorrect':\n",
    "            language_reference_map[id_] = False\n",
    "        else:\n",
    "            language_reference_map[id_] = True\n",
    "            \n",
    "        if meaning == 'incorrect':\n",
    "            meaning_reference_map[id_] = False\n",
    "        else:\n",
    "            meaning_reference_map[id_] = True"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
