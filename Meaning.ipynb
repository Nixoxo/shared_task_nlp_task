{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import spacy\n",
    "nlp_en = spacy.load('en')\n",
    "nlp_de = spacy.load('de')\n",
    "from spacy.en import English\n",
    "parser = English()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from spacy.en import English\n",
    "parser = English()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_prompt_response_map(gram):\n",
    "    prompt_response_map = {}\n",
    "    for prompt_unit in gram:\n",
    "        key = None\n",
    "        responses = []\n",
    "        for prompt in prompt_unit:\n",
    "            if prompt.tag == None:\n",
    "                print(\"None\")\n",
    "            else:\n",
    "                if(prompt.tag == 'prompt'):\n",
    "                    key = prompt.text\n",
    "                elif(prompt.tag == 'response'):\n",
    "                    responses.append(prompt.text)\n",
    "        prompt_response_map[key] = responses\n",
    "    return prompt_response_map\n",
    "\n",
    "def read_grammar_and_create_map(file):\n",
    "    tree = ET.parse(file)\n",
    "    grammar = tree.getroot()\n",
    "    return create_prompt_response_map(grammar)\n",
    "\n",
    "def get_sag_prompts(prompt_response_map):\n",
    "    sag_prompts = {}\n",
    "    for key in prompt_response_map:\n",
    "        if \"Sag\" in key:\n",
    "            sag_prompts[key] = prompt_response_map[key]\n",
    "    return sag_prompts\n",
    "\n",
    "    \n",
    "grammar = read_grammar_and_create_map('referenceGrammar.xml')\n",
    "#sag_prompts = get_sag_prompts(prompt_response_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_file_csv = \"textProcessing_testKaldi.csv\"\n",
    "test_data = None\n",
    "with open(test_file_csv, 'r') as reader:\n",
    "    test_data = reader.readlines()  \n",
    "    \n",
    "def prompt_map_test_data():\n",
    "    prompt_map = {}\n",
    "    for item in test_data[1:]:\n",
    "        split = item.replace(\"\\n\", \"\").split(\"\\t\")\n",
    "        prompt = split[1]\n",
    "        \n",
    "        if prompt in prompt_map:\n",
    "            prompt_map[prompt].append({'id': split[0], \"transcript\": split[3], \"prompt\": split[1]})\n",
    "        else:\n",
    "            arr = []\n",
    "            arr.append({'id':split[0], \"transcript\":split[3], \"prompt\": split[1]})\n",
    "            prompt_map[prompt] = arr\n",
    "            \n",
    "    return prompt_map\n",
    "prompt_test_map = prompt_map_test_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Basis based on Reference Grammar and Normal Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "special_case_map = {\"don't\": \"do not\",\n",
    " \"haven't\" : \"have not\",\n",
    " \"i'd\" : \"i would\",\n",
    " \"i'm\" : \"i am\",\n",
    " \"isn't\": \"is not\",\n",
    " \"it's\": \"it is\",\n",
    " \"o'clock\": \"o'clock\",\n",
    " \"that's\" :\"that is\",\n",
    " \"there's\": \"there is\"}\n",
    "\n",
    "def sentence_to_normal_case(sentence):\n",
    "    trans_sentence = \"\"\n",
    "    words = sentence.split(\" \")\n",
    "    for word in words:\n",
    "        if word in special_case_map:\n",
    "            word = special_case_map[word]\n",
    "        trans_sentence += word + \" \"\n",
    "    \n",
    "    return trans_sentence[:-1]\n",
    "\n",
    "meaning_false = {}\n",
    "for prompt_unit in prompt_test_map:\n",
    "    for response in prompt_test_map[prompt_unit]:\n",
    "        \n",
    "        sentence = response['transcript']\n",
    "        if \"***\" in sentence:\n",
    "            continue\n",
    "        if \"'\" in sentence:\n",
    "            sentence = sentence_to_normal_case(sentence)\n",
    "            #print(dict_prompt['transcript'])\n",
    "            \n",
    "        sentence = ' '.join(sentence.split())\n",
    "        if sentence not in prompt_response_map[prompt_unit]:\n",
    "            meaning_false[response['id']] = {\"sentence\":sentence, \"prompt\":response['prompt']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prompt_meaning_false = {}\n",
    "for id_ in meaning_false:\n",
    "    for item in meaning_false[id_]:\n",
    "        prompt_key = meaning_false[id_][\"prompt\"]\n",
    "        if prompt_key in prompt_meaning_false:\n",
    "            prompt_meaning_false[prompt_key].append(meaning_false[id_][\"sentence\"])\n",
    "        else:\n",
    "            arr = []\n",
    "            arr.append(meaning_false[id_][\"sentence\"])\n",
    "            prompt_meaning_false[prompt_key] = arr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Meaning Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nlp_sentence(sentence):\n",
    "    parsed = parser(sentence)\n",
    "    lemmas = []\n",
    "    words = []\n",
    "    tags = []\n",
    "    poss = []\n",
    "    for i, token in enumerate(parsed):\n",
    "        lemma = token.lemma_\n",
    "        words.append(token)\n",
    "        lemmas.append(lemma)\n",
    "        tag = token.tag_\n",
    "        pos = token.tag_\n",
    "        tags.append(tag)\n",
    "        poss.append(pos)\n",
    "    return words, lemmas, tags, poss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(meaning_false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_key = \"Sag: Ich habe keine Reservation\"\n",
    "prompt_meaning_false[test_key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def replace_sentence_emp_words(sentence):\n",
    "    return sentence.replace(\"thank you \", \"\")\\\n",
    "    .replace(\" please\", \"\")\\\n",
    "    .replace(\"yes \", \"\")\\\n",
    "    .replace(\"thank you \", \"\")\\\n",
    "    .replace(\"no \", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def spacy_words_to_string_array(words):\n",
    "    arr = []\n",
    "    for word in words:\n",
    "        arr.append(str(word))\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_key_nouns(nlp_sent):\n",
    "    tags = nlp_sent[2]\n",
    "    words = spacy_words_to_string_array(nlp_sent[0])\n",
    "    sentence = \" \".join(words) + \" \"\n",
    "    #words = \" \".join(nlp_sent[0])\n",
    "\n",
    "    tags_string = \" \".join(tags)\n",
    "    patterns = [['CD', 'JJ', 'NN'], ['JJ', 'NN'], ['DT', 'NN'], ['CD', 'JJ', 'NNS'], ['JJ', 'NNS'], ['DT', 'NNS']]\n",
    "    extracted_words = []\n",
    "    for pattern in patterns:\n",
    "        pattern_string = \" \".join(pattern) + \" \"\n",
    "        if pattern_string in tags_string: \n",
    "            if pattern[-1] not in tags and pattern[0] not in tags:\n",
    "                continue\n",
    "            pattern_start_index = tags.index(pattern[0])\n",
    "            pattern_end_index = tags.index(pattern[-1])\n",
    "            if pattern_end_index - pattern_start_index + 1 == len(pattern):\n",
    "                extracted_part = \"\"\n",
    "                for i in range(pattern_start_index, pattern_end_index+1):\n",
    "                    extracted_part += words[i] + \" \"\n",
    "                \n",
    "                extracted_words.append(extracted_part[:-1])\n",
    "    return extracted_words\n",
    "\n",
    "#print(nlp_sentence(\"i am looking for one musical ticket\")[2])    \n",
    "extract_key_nouns(nlp_sentence(\"i am looking for one musical ticket\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "counter = 0\n",
    "result = []\n",
    "for sentence in grammar[test_key]:\n",
    "    replaced_sentence = replace_sentence_emp_words(sentence)\n",
    "    nlp_sent = nlp_sentence(replaced_sentence)\n",
    "    tags = nlp_sent[2]\n",
    "    if tags[0] == 'PRP':\n",
    "        counter +=1\n",
    "        result.extend(extract_key_nouns(nlp_sent))\n",
    "        print(str(spacy_words_to_string_array(nlp_sent[0])))\n",
    "        print(nlp_sent[2])\n",
    "print(counter)\n",
    "print(len(result))\n",
    "print(set(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_key_verbs_by_prp(nlp_sent):\n",
    "    tags = nlp_sent[2]\n",
    "    words = spacy_words_to_string_array(nlp_sent[0])\n",
    "    sentence = \" \".join(words)\n",
    "    #words = \" \".join(nlp_sent[0])\n",
    "\n",
    "    tags_string = \" \".join(tags)\n",
    "    prio_one_patterns = [['VBP', 'VBG', 'IN'], ['MD', 'VB', 'CD'], ['VBP', 'RB']]\n",
    "    # Only when no prio_one_pattern fits\n",
    "    prio_sec_patterns = [['VBP']]\n",
    "    match = False\n",
    "    extracted_words = []\n",
    "    for pattern in prio_one_patterns:\n",
    "        pattern_string = \" \".join(pattern)\n",
    "        if pattern_string in tags_string:  \n",
    "            pattern_start_index = tags.index(pattern[0])\n",
    "            pattern_end_index = tags.index(pattern[-1])\n",
    "            if pattern_end_index - pattern_start_index + 1 == len(pattern):\n",
    "                extracted_part = \"\"\n",
    "                for i in range(pattern_start_index, pattern_end_index+1):\n",
    "                    extracted_part += words[i] + \" \"\n",
    "                \n",
    "                extracted_words.append(extracted_part[:-1])\n",
    "    if len(extracted_words) <1:\n",
    "        for pattern in prio_sec_patterns:\n",
    "            pattern_string = \" \".join(pattern)\n",
    "            if pattern_string in tags_string:  \n",
    "                pattern_start_index = tags.index(pattern[0])\n",
    "                pattern_end_index = tags.index(pattern[-1])\n",
    "                if pattern_end_index - pattern_start_index + 1 == len(pattern):\n",
    "                    extracted_part = \"\"\n",
    "                    for i in range(pattern_start_index, pattern_end_index+1):\n",
    "                        extracted_part += words[i] + \" \"\n",
    "\n",
    "                    extracted_words.append(extracted_part[:-1])\n",
    "        \n",
    "    return extracted_words\n",
    "\n",
    "extract_key_verbs_by_prp(nlp_sentence(\"i am looking for one musical ticket\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_key_questions(nlp_sent):\n",
    "    tags = nlp_sent[2]\n",
    "    words = spacy_words_to_string_array(nlp_sent[0])\n",
    "    sentence = \" \".join(words)\n",
    "    #words = \" \".join(nlp_sent[0])\n",
    "\n",
    "    tags_string = \" \".join(tags)\n",
    "    prio_one_patterns = [['MD', 'PRP', 'VB'], ['VBP', 'PRP', 'VBN'], ['VBP', 'PRP', 'VB']]\n",
    "    # Only when no prio_one_pattern fits\n",
    "    #prio_sec_patterns = [['VBP']]\n",
    "    match = False\n",
    "    extracted_words = []\n",
    "    for pattern in prio_one_patterns:\n",
    "        pattern_string = \" \".join(pattern) + \" \"\n",
    "        if pattern_string in tags_string:\n",
    "            if pattern[0] not in tags and pattern[-1] not in tags:\n",
    "                continue\n",
    "            pattern_start_index = tags.index(pattern[0])\n",
    "            pattern_end_index = tags.index(pattern[-1])\n",
    "            if pattern_end_index - pattern_start_index + 1 == len(pattern):\n",
    "                extracted_part = \"\"\n",
    "                for i in range(pattern_start_index, pattern_end_index+1):\n",
    "                    extracted_part += words[i] + \" \"\n",
    "                \n",
    "                extracted_words.append(extracted_part[:-1])\n",
    "        \n",
    "    return extracted_words\n",
    "\n",
    "_test_key = \"have you got one musical ticket\"\n",
    "#print(nlp_sentence(_test_key)[2])\n",
    "extract_key_questions(nlp_sentence(_test_key))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "counter = 0\n",
    "result_prp = []\n",
    "result_md = []\n",
    "for sentence in grammar[test_key]:\n",
    "    replaced_sentence = replace_sentence_emp_words(sentence)\n",
    "    nlp_sent = nlp_sentence(replaced_sentence)\n",
    "    tags = nlp_sent[2]\n",
    "    if tags[0] != 'PRP':\n",
    "        extracted = extract_key_questions(nlp_sent)\n",
    "        if len(extracted) < 1:\n",
    "            print(sentence)\n",
    "            print(nlp_sent[2])\n",
    "            #print(extract_key_verbs_by_prp(nlp_sent))\n",
    "        else:\n",
    "            result_md.extend(extracted)\n",
    "            \n",
    "\n",
    "        #print(str(spacy_words_to_string_array(nlp_sent[0])))\n",
    "        #print(nlp_sent[2])\n",
    "print(counter)\n",
    "print(len(result_prp))\n",
    "print(set(result_md))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine NN, Q, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "extract_key_nouns(nlp_sentence(\"i do not have a reservation\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for sent in errors_nouns:\n",
    "    n_s = nlp_sentence(sent)\n",
    "    try:    \n",
    "        extract_key_nouns(n_s)\n",
    "    except:\n",
    "        print(sent)\n",
    "        print(n_s)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "verbs = []\n",
    "nouns = []\n",
    "errors_nouns = []\n",
    "errors_verbs = []\n",
    "for key in grammar:\n",
    "    for response in grammar[key]:\n",
    "        replaced_sentence = replace_sentence_emp_words(response)\n",
    "        nlp_sent = nlp_sentence(replaced_sentence)  \n",
    "        tags = nlp_sent[2]\n",
    "        if tags[0] == 'PRP':\n",
    "            try:\n",
    "                verbs.extend(extract_key_verbs_by_prp(nlp_sent))\n",
    "            except:\n",
    "                errors_verbs.append(response)\n",
    "            try:    \n",
    "                nouns.extend(extract_key_nouns(nlp_sent))\n",
    "            except:\n",
    "                errors_nouns.append(response)\n",
    "            \n",
    "        \n",
    "print(len(set(verbs)))\n",
    "print(len(set(nouns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for verb in set(verbs):\n",
    "    print(verb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "verbs = []\n",
    "nouns = []\n",
    "for response in grammar[test_key]:\n",
    "    replaced_sentence = replace_sentence_emp_words(response)\n",
    "    nlp_sent = nlp_sentence(replaced_sentence)  \n",
    "    tags = nlp_sent[2]\n",
    "    if tags[0] == 'PRP':\n",
    "        verbs.extend(extract_key_verbs_by_prp(nlp_sent))\n",
    "        nouns.extend(extract_key_nouns(nlp_sent))\n",
    "        \n",
    "        \n",
    "print(set(verbs))\n",
    "print(set(nouns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for item in prompt_test_map[test_key]:\n",
    "    if item['transcript'] not in grammar[test_key]:\n",
    "        print(item['transcript'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sag_prompts = get_sag_prompts(grammar)\n",
    "for key in sag_prompts:\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_prompts = [\"Sag: Ich möchte mit Dollars bezahlen\",\n",
    "\"Sag: Ich möchte mit Euros bezahlen\",\n",
    "\"Sag: Ich möchte mit Kreditkarte bezahlen\",\n",
    "\"Sag: Ich möchte mit Mastercard bezahlen\",\n",
    "\"Sag: Ich möchte mit Postkarte bezahlen\",\n",
    "\"Sag: Ich möchte mit Visa bezahlen\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_prompts_sentences = []\n",
    "counter_max = 0\n",
    "max_prompt = \"\"\n",
    "for test_prompt in test_prompts:\n",
    "    tp_prompts = grammar[test_prompt]\n",
    "    if len(tp_prompts) > counter_max:\n",
    "        max_prompt = test_prompt\n",
    "        counter_max = len(tp_prompts)\n",
    "        \n",
    "    test_prompts_sentences.extend(tp_prompts)\n",
    "print(len(test_prompts_sentences))\n",
    "print(counter_max)\n",
    "print(max_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_slice_in_list(s,l):\n",
    "    len_s = len(s) #so we don't recompute length of s on every iteration\n",
    "    return any(s == l[i:len_s+i] for i in range(len(l) - len_s+1))\n",
    "\n",
    "def extract_by_pattern(patterns, tags, words):\n",
    "    extracted_words = []\n",
    "    tags_string = \" \".join(tags)\n",
    "    for pattern in patterns:\n",
    "        pattern_string = \" \".join(pattern)\n",
    "        if is_slice_in_list(pattern, tags):\n",
    "            pattern_start_index = tags.index(pattern[0])\n",
    "            if len(pattern) > 1:\n",
    "                pattern_end_index = tags.index(pattern[-1], pattern_start_index+1)\n",
    "                if pattern_end_index - pattern_start_index + 1 == len(pattern):\n",
    "                    extracted_part = \"\"\n",
    "                    for i in range(pattern_start_index, pattern_end_index+1):\n",
    "                        extracted_part += words[i] + \" \"\n",
    "\n",
    "                    extracted_words.append(extracted_part[:-1])\n",
    "            else:\n",
    "                extracted_words.append(words[pattern_start_index])\n",
    "    return extracted_words\n",
    "    \n",
    "def extract_key_nouns_credit_card(nlp_sent):\n",
    "    tags = nlp_sent[2]\n",
    "    words = spacy_words_to_string_array(nlp_sent[0])\n",
    "    sentence = \" \".join(words)\n",
    "    #words = \" \".join(nlp_sent[0])\n",
    "\n",
    "    prio_one_patterns = [['NN', 'NNS'], ['NN', 'NN']]\n",
    "    \n",
    "    # Only when no prio_one_pattern fits\n",
    "    prio_sec_patterns = [['NN'], ['NNS']]\n",
    "    match = False\n",
    "    extracted_words = extract_by_pattern(prio_one_patterns, tags, words)\n",
    "    if len(extracted_words) < 1:\n",
    "        extracted_words = extract_by_pattern(prio_sec_patterns, tags, words)\n",
    "    \n",
    "    return extracted_words\n",
    "\n",
    "def extract_key_verbs_question_credit_card(nlp_sent):\n",
    "    tags = nlp_sent[2]\n",
    "    words = spacy_words_to_string_array(nlp_sent[0])\n",
    "    sentence = \" \".join(words)\n",
    "    #words = \" \".join(nlp_sent[0])\n",
    "    \n",
    "    prio_one_question_patterns = [['VBZ', 'PRP', 'JJ', 'TO', 'VB']]\n",
    "    prio_two_question_patterns = [['MD', 'PRP', 'VB', 'IN']]\n",
    "    prio_three_question_patterns = [['MD', 'PRP', 'VB'], ['VBP', 'PRP', 'VB']]\n",
    "    \n",
    "    #prio_one_patterns = [['VBP', 'VBG', 'IN'], ['MD', 'VB', 'CD'], ['VBP', 'RB']]\n",
    "    # Only when no prio_one_pattern fits\n",
    "    #prio_sec_patterns = [['VBP']]\n",
    "\n",
    "    #prio_one_patterns = [['NN', 'NNS'], ['NN', 'NN']]\n",
    "    \n",
    "    # Only when no prio_one_pattern fits\n",
    "    #prio_sec_patterns = [['NN'], ['NNS']]\n",
    "    match = False\n",
    "    extracted_words = extract_by_pattern(prio_one_question_patterns, tags, words)\n",
    "    if len(extracted_words) < 1:\n",
    "        extracted_words = extract_by_pattern(prio_two_question_patterns, tags, words)\n",
    "        if len(extracted_words) < 1:\n",
    "            extracted_words = extract_by_pattern(prio_three_question_patterns, tags, words)\n",
    "    \n",
    "    return extracted_words\n",
    "\n",
    "def extract_key_verbs_credit_card(nlp_sent):\n",
    "    tags = nlp_sent[2]\n",
    "    words = spacy_words_to_string_array(nlp_sent[0])\n",
    "    sentence = \" \".join(words)\n",
    "    #words = \" \".join(nlp_sent[0])\n",
    "    \n",
    "    prio_one = [['MD', 'VB', 'TO', 'VB', 'IN']]\n",
    "\n",
    "    \n",
    "    #prio_one_patterns = [['VBP', 'VBG', 'IN'], ['MD', 'VB', 'CD'], ['VBP', 'RB']]\n",
    "    # Only when no prio_one_pattern fits\n",
    "    #prio_sec_patterns = [['VBP']]\n",
    "\n",
    "    #prio_one_patterns = [['NN', 'NNS'], ['NN', 'NN']]\n",
    "    \n",
    "    # Only when no prio_one_pattern fits\n",
    "    #prio_sec_patterns = [['NN'], ['NNS']]\n",
    "    match = False\n",
    "    extracted_words = extract_by_pattern(prio_one, tags, words)\n",
    "    \"\"\"if len(extracted_words) < 1:\n",
    "        extracted_words = extract_by_pattern(prio_two_question_patterns, tags, words)\n",
    "        if len(extracted_words) < 1:\n",
    "            extracted_words = extract_by_pattern(prio_three_question_patterns, tags, words)\n",
    "    \"\"\"\n",
    "    return extracted_words\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([can, i, pay, by, card], ['can', 'i', 'pay', 'by', 'card'], ['MD', 'PRP', 'VB', 'IN', 'NN'], ['MD', 'PRP', 'VB', 'IN', 'NN'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['can i pay by']"
      ]
     },
     "execution_count": 472,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testtt = \"can i pay by card\"\n",
    "print(nlp_sentence(testtt))\n",
    "extract_key_verbs_question_credit_card(nlp_sentence(testtt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "testtt = \"can i pay by credit card\"\n",
    "print(nlp_sentence(testtt))\n",
    "extract_key_nouns_credit_card(nlp_sentence(testtt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_key = \"Sag: Ich möchte mit Kreditkarte bezahlen\"\n",
    "verbs = []\n",
    "pattern_verbs = []\n",
    "for item in grammar[test_key]:\n",
    "    nlp_se = nlp_sentence(item)\n",
    "    verb_ques = extract_key_verbs_question_credit_card(nlp_se)\n",
    "    if len(verb_ques) > 0:\n",
    "        verbs.extend(verb_ques)\n",
    "    else:\n",
    "        print(item)\n",
    "        verb = extract_key_verbs_credit_card(nlp_se)\n",
    "        if len(verb) == 0:\n",
    "            pattern_verbs.append(nlp_se[2])\n",
    "        else:\n",
    "            pattern_verbs.append(verb)\n",
    "\n",
    "print(set(verbs))\n",
    "print(pattern_verbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nouns = []\n",
    "question_verbs = []\n",
    "verbs = []\n",
    "for key in test_prompts:\n",
    "    print(key)\n",
    "    for item in grammar[key]:\n",
    "        nlp_se = nlp_sentence(item)\n",
    "        \n",
    "        nouns.extend(extract_key_nouns_credit_card(nlp_se))\n",
    "        question_verbs.extend(extract_key_verbs_question_credit_card(nlp_se))\n",
    "        verbs.extend(extract_key_verbs_credit_card(nlp_se))\n",
    "print(set(nouns))\n",
    "print(set(question_verbs))\n",
    "print(set(verbs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nouns_list = list(set(verbs))\n",
    "print(nouns_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['can i buy by post ',\n",
       " 'i would to pay by dollars',\n",
       " 'i would pay with euros',\n",
       " 'can i   am a    room',\n",
       " 'my',\n",
       " 'i want to pay with credit card',\n",
       " 'i want to pay with by credit card',\n",
       " 'i want to pay with  ***  credit card',\n",
       " ' i would like pay with by credit card',\n",
       " 'i would to pay by credit card',\n",
       " 'i would to pay by credit card',\n",
       " 'i would to pay pay by credit card',\n",
       " 'i would pay by credit card',\n",
       " 'i   ***  would pay with credit card',\n",
       " 'i want where is the credit card',\n",
       " 'i would pay with credit card',\n",
       " 'i would like pay with the credit card',\n",
       " 'i want pay with credit card',\n",
       " 'i want pay with credit card',\n",
       " 'i like to buy with credit card',\n",
       " 'i like to pay by credit card',\n",
       " 'i like to pay with credit card',\n",
       " 'i like to pay with credit card',\n",
       " 'can i  ***  will credit card',\n",
       " \" i don't note\",\n",
       " 'can i   am a pay by mastercard',\n",
       " 'can i pay by    ***   mastercard',\n",
       " 'i would like to pay with my    ***   mastercard',\n",
       " 'i   ***    ***  pay with mastercard',\n",
       " '***   ***   ***     ***  ',\n",
       " 'i would like to pay with master card   ',\n",
       " 'can i pay by    ***   mastercard',\n",
       " 'i want to buy with    ***   mastercard',\n",
       " 'can i pay with leave at card   ',\n",
       " 'i want pay with is the mastercard',\n",
       " 'can i pay be mastercard',\n",
       " 'can i  ***   my mastercard',\n",
       " 'i want to pay with my post card',\n",
       " 'i would like to pay with by post card',\n",
       " 'i would like to pay with',\n",
       " 'i want pay',\n",
       " 'i want to pay with credit card',\n",
       " 'i would like to pay by master card',\n",
       " \"i'd like to pay with post card\",\n",
       " 'i would pay with post card',\n",
       " 'i like  ***  pay with post card',\n",
       " '***   can i   am by post card',\n",
       " 'i would like  ***  ***   pay visa',\n",
       " 'i would like to pay  ***  visa',\n",
       " 'i would like to  ***   pay visa',\n",
       " 'can i pay with  ***  visa',\n",
       " 'i want to pay by visa',\n",
       " 'i would to pay by visa',\n",
       " 'i go pay with green park',\n",
       " 'pay with visa',\n",
       " 'i want pay with visa card',\n",
       " 'i want pay with visa',\n",
       " 'i want pay with visa',\n",
       " 'can i pay with visa visa']"
      ]
     },
     "execution_count": 468,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_prompts_in_test_data = []\n",
    "for key in test_prompts:\n",
    "    test_prompts_in_test_data.append(prompt_test_map[key])\n",
    "\n",
    "test_prompts_in_test_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "counter = 0\n",
    "credit_test_prompts = []\n",
    "for key in test_prompts:\n",
    "    #counter += len((prompt_test_map[key]))\n",
    "    for chid_reponse in prompt_test_map[key]:\n",
    "        \n",
    "        if chid_reponse['transcript'] not in grammar[key]:\n",
    "            credit_test_prompts.append(chid_reponse['transcript'])\n",
    "\n",
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "credit_test_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generalise_aux_verb(nlp_sent):\n",
    "    tags = nlp_sent[2]\n",
    "    words = spacy_words_to_string_array(nlp_sent[0])\n",
    "    sentence = \" \".join(words)\n",
    "\n",
    "    prio_one = [['PRP', 'MD', 'VB', 'TO']]\n",
    "    prio_two = [['PRP', 'VBP', 'TO']]\n",
    "\n",
    "    match = False\n",
    "    extracted_words = extract_by_pattern(prio_one, tags, words)\n",
    "    if len(extracted_words) < 1:\n",
    "        extracted_words = extract_by_pattern(prio_two, tags, words)\n",
    "        #if len(extracted_words) < 1:\n",
    "        #    extracted_words = extract_by_pattern(prio_three_question_patterns, tags, words)\n",
    "    return extracted_words \n",
    "\n",
    "def generalise_verb_prep(nlp_sent):\n",
    "    tags = nlp_sent[2]\n",
    "    words = spacy_words_to_string_array(nlp_sent[0])\n",
    "    sentence = \" \".join(words)\n",
    "    #words = \" \".join(nlp_sent[0])\n",
    "    prio_one = [['VBP', 'IN'], ['VB', 'IN']]\n",
    "    #prio_one_patterns = [['VBP', 'VBG', 'IN'], ['MD', 'VB', 'CD'], ['VBP', 'RB']]\n",
    "    # Only when no prio_one_pattern fits\n",
    "    #prio_sec_patterns = [['VBP']]\n",
    "\n",
    "    #prio_one_patterns = [['NN', 'NNS'], ['NN', 'NN']]\n",
    "    \n",
    "    # Only when no prio_one_pattern fits\n",
    "    #prio_sec_patterns = [['NN'], ['NNS']]\n",
    "    match = False\n",
    "    extracted_words = extract_by_pattern(prio_one, tags, words)\n",
    "    \"\"\"if len(extracted_words) < 1:\n",
    "        extracted_words = extract_by_pattern(prio_two, tags, words)\n",
    "        #if len(extracted_words) < 1:\n",
    "        #    extracted_words = extract_by_pattern(prio_three_question_patterns, tags, words)\n",
    "    \"\"\"\n",
    "    return extracted_words \n",
    "\n",
    "def generalise_verb_det(nlp_sent):\n",
    "    tags = nlp_sent[2]\n",
    "    words = spacy_words_to_string_array(nlp_sent[0])\n",
    "    sentence = \" \".join(words)\n",
    "    #words = \" \".join(nlp_sent[0])\n",
    "    prio_one = [['VBP', 'DT'], ['VB', 'DT']]\n",
    "    #prio_one_patterns = [['VBP', 'VBG', 'IN'], ['MD', 'VB', 'CD'], ['VBP', 'RB']]\n",
    "    # Only when no prio_one_pattern fits\n",
    "    #prio_sec_patterns = [['VBP']]\n",
    "\n",
    "    #prio_one_patterns = [['NN', 'NNS'], ['NN', 'NN']]\n",
    "    \n",
    "    # Only when no prio_one_pattern fits\n",
    "    #prio_sec_patterns = [['NN'], ['NNS']]\n",
    "    match = False\n",
    "    extracted_words = extract_by_pattern(prio_one, tags, words)\n",
    "    \"\"\"if len(extracted_words) < 1:\n",
    "        extracted_words = extract_by_pattern(prio_two, tags, words)\n",
    "        #if len(extracted_words) < 1:\n",
    "        #    extracted_words = extract_by_pattern(prio_three_question_patterns, tags, words)\n",
    "    \"\"\"\n",
    "    return extracted_words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "found_aux_verb = []\n",
    "found_verb_prep = []\n",
    "found_verb_det = []\n",
    "for key in grammar:\n",
    "    for item in grammar[key]:\n",
    "        nlp_s = nlp_sentence(item)\n",
    "        aux_verb = generalise_aux_verb(nlp_s)\n",
    "        if len(aux_verb) > 0:\n",
    "            found_aux_verb.extend(aux_verb)\n",
    "            \n",
    "        verb_prep = generalise_verb_prep(nlp_s)\n",
    "        if len(verb_prep) > 0:\n",
    "            found_verb_prep.extend(verb_prep)\n",
    "            \n",
    "        verb_det = generalise_verb_det(nlp_s)\n",
    "        if len(verb_det) > 0:\n",
    "            found_verb_det.extend(verb_det)\n",
    "        \n",
    "print(len(set(found_aux_verb)))\n",
    "print(len(set(found_verb_prep)))\n",
    "print(len(set(found_verb_det)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(len(set(found_aux_verb)))\n",
    "print(set(found_aux_verb))\n",
    "print(len(set(found_verb_prep)))\n",
    "print(set(found_verb_prep))\n",
    "print(len(set(found_verb_det)))\n",
    "print(set(found_verb_det))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(list(set(found_verb_det)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentence = \"i 'd like to pay with post card\"\n",
    "\n",
    "\n",
    "def aux_key(sentence):\n",
    "    for auxkey in set(found_aux_verb):\n",
    "        if auxkey in sentence:\n",
    "            return auxkey\n",
    "\n",
    "\n",
    "def verb_prep_key(sentence):\n",
    "    for verbprepkey in set(found_verb_prep):\n",
    "        if verbprepkey in sentence:\n",
    "            return verbprepkey\n",
    "        \n",
    "def inside_nouns(sentence):\n",
    "    for nounkey in sorted(set(nouns), key=len, reverse=True):\n",
    "        if nounkey in sentence:\n",
    "            return nounkey\n",
    "        \n",
    "def accept_credit_card(sentence):\n",
    "    aux = aux_key(sentence)\n",
    "    if aux:\n",
    "        sentence = sentence.replace(aux, \"\")\n",
    "        verb_prep = verb_prep_key(sentence)\n",
    "        if verb_prep:\n",
    "            sentence = sentence.replace(verb_prep, \"\")\n",
    "            sentence= \" \".join(sentence.split(None))\n",
    "            nounk = inside_nouns(sentence)\n",
    "            if nounk:\n",
    "                sentence = sentence.replace(nounk, \"\")\n",
    "                if len(sentence) == 0:\n",
    "                    return True\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "accept_credit_card(\"i'd like to pay with post card\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(\"i'd like to pay with post card\".replace(\"'\", \" '\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "counter = 0\n",
    "for pro in credit_test_prompts:\n",
    "    sentence = \" \".join(pro.split(None))\n",
    "    if \"'\" in sentence:\n",
    "        sentence = sentence.replace(\"'\", \" '\")\n",
    "    if accept_credit_card(sentence) == False:\n",
    "        counter +=1\n",
    "        print(pro)\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "generalise_aux_verb(nlp_sentence(\"i would like to pay with master card\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nlp_sentence(\"i would need to go tomorrow evening\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "generalise_verb_prep(nlp_sentence(\"i pay by visa\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "generalise_verb_det(nlp_sentence(\"do you have a ticket to westminster\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generalize it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71"
      ]
     },
     "execution_count": 564,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_prompts = [\"Sag: Ich möchte mit Dollars bezahlen\",\n",
    "\"Sag: Ich möchte mit Euros bezahlen\",\n",
    "\"Sag: Ich möchte mit Kreditkarte bezahlen\",\n",
    "\"Sag: Ich möchte mit Mastercard bezahlen\",\n",
    "\"Sag: Ich möchte mit Postkarte bezahlen\",\n",
    "\"Sag: Ich möchte mit Visa bezahlen\",\n",
    "\"Sag: Ich möchte mit Pfund bezahlen\",\n",
    "\"Sag: Ich möchte mit Schweizer Franken bezahlen\"]\n",
    "\n",
    "def get_test_prompts_transcripts_by_cluster(cluster_prompts, prompt_test_data_map):\n",
    "    # Hole alle Daten mit den prompts\n",
    "    transcripts_arr = []\n",
    "    for key in cluster_prompts:\n",
    "        for response in prompt_test_data_map[key]:\n",
    "            if response['transcript'] not in grammar[key]:\n",
    "                transcripts_arr.append(response['transcript'])\n",
    "    #for key in arr:\n",
    "    #    if arr[]\n",
    "    #    transcripts_arr.append()\n",
    "    return transcripts_arr\n",
    "    \n",
    "len(get_test_prompts_transcripts_by_cluster(test_prompts, prompt_test_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sag: Ich möchte mit Dollars bezahlen\n",
      "Sag: Ich möchte mit Euros bezahlen\n",
      "Sag: Ich möchte mit Kreditkarte bezahlen\n",
      "Sag: Ich möchte mit Mastercard bezahlen\n",
      "Sag: Ich möchte mit Postkarte bezahlen\n",
      "Sag: Ich möchte mit Visa bezahlen\n",
      "Sag: Ich möchte mit Pfund bezahlen\n",
      "Sag: Ich möchte mit Schweizer Franken bezahlen\n",
      "['card', 'credit card', 'pounds', 'mastercard', 'visa', 'francs', 'dollars', 'cards', 'master card', 'euros', 'credit cards', 'post card']\n",
      "['is it possible to pay', 'do you accept', 'can i pay with', 'can i pay', 'can i pay by']\n",
      "['would like to pay with', 'would like to pay by']\n"
     ]
    }
   ],
   "source": [
    "def generate_nouns_etc(cluster_prompts):\n",
    "    nouns = []\n",
    "    question_verbs = []\n",
    "    verbs = []\n",
    "    for key in cluster_prompts:\n",
    "        print(key)\n",
    "        for item in grammar[key]:\n",
    "            nlp_se = nlp_sentence(item)\n",
    "\n",
    "            nouns.extend(extract_key_nouns_credit_card(nlp_se))\n",
    "            question_verbs.extend(extract_key_verbs_question_credit_card(nlp_se))\n",
    "            verbs.extend(extract_key_verbs_credit_card(nlp_se))\n",
    "    print(list(set(nouns)))\n",
    "    print(list(set(question_verbs)))\n",
    "    print(list(set(verbs)))\n",
    "generate_nouns_etc(test_prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence = \"i 'd like to pay with post card\"\n",
    "\n",
    "def aux_key(sentence):\n",
    "    for auxkey in set(found_aux_verb):\n",
    "        if auxkey in sentence:\n",
    "            return auxkey\n",
    "\n",
    "\n",
    "def verb_prep_key(sentence):\n",
    "    for verbprepkey in set(found_verb_prep):\n",
    "        if verbprepkey in sentence:\n",
    "            return verbprepkey\n",
    "        \n",
    "def inside_nouns(sentence):\n",
    "    for nounkey in sorted(set(nouns), key=len, reverse=True):\n",
    "        if nounkey in sentence:\n",
    "            return nounkey\n",
    "        \n",
    "def accept_credit_card(sentence):\n",
    "    aux = aux_key(sentence)\n",
    "    if aux:\n",
    "        sentence = sentence.replace(aux, \"\")\n",
    "        verb_prep = verb_prep_key(sentence)\n",
    "        if verb_prep:\n",
    "            sentence = sentence.replace(verb_prep, \"\")\n",
    "            sentence= \" \".join(sentence.split(None))\n",
    "            nounk = inside_nouns(sentence)\n",
    "            if nounk:\n",
    "                sentence = sentence.replace(nounk, \"\")\n",
    "                if len(sentence) == 0:\n",
    "                    return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\ti would like to pay with master with master card\n",
      "False\ti would like to pay by have from france\n",
      "False\tcan i  ***   am from france\n",
      "False\t***   can i   i buy three nights\n",
      "False\ti like i   with swiss francs\n"
     ]
    }
   ],
   "source": [
    "for sent in get_test_prompts_transcripts_by_cluster([\"Sag: Ich möchte mit Schweizer Franken bezahlen\"], prompt_test_map):\n",
    "    print(str(accept_credit_card(sent)) + \"\\t\" + sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i would like to pay with master with master card',\n",
       " 'i would like to pay by have from france',\n",
       " 'can i  ***   am from france',\n",
       " '***   can i   i buy three nights',\n",
       " 'i like i   with swiss francs']"
      ]
     },
     "execution_count": 487,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(get_test_prompts_transcripts_by_cluster([\"Sag: Ich möchte mit Schweizer Franken bezahlen\"], prompt_test_map))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster Restarant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i would like the bill',\n",
       " 'i would like to pay',\n",
       " 'i would like the check',\n",
       " 'can i have the bill',\n",
       " 'can i have the bill',\n",
       " 'can i have the bill',\n",
       " ' no i have you the gym',\n",
       " 'yes can i have the dessert card',\n",
       " 'can i have the dessert card',\n",
       " 'i would like the dessert card',\n",
       " 'i would like the is there card',\n",
       " 'i would like a    ticket to ',\n",
       " ' i dessert menu',\n",
       " 'can i have a   dessert card',\n",
       " 'i would like to the dessert card',\n",
       " 'can i have the dessert menu',\n",
       " 'can i have the dessert menu    *** ',\n",
       " 'can i have the dessert menu',\n",
       " 'can i have the dessert my room',\n",
       " 'can i   have the dessert menu please can i have the dessert menu please',\n",
       " 'can i have the dessert menu',\n",
       " 'can i have the dessert menu',\n",
       " 'can i have the dessert menu pool ',\n",
       " 'can i have the    ***    swimming pool ']"
      ]
     },
     "execution_count": 491,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_prompts= [\"Frag: Ich möchte die Rechnung\", \"Frag: Ich möchte die Dessertkarte\"]\n",
    "list(get_test_prompts_transcripts_by_cluster(test_prompts, prompt_test_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['would you bring me']"
      ]
     },
     "execution_count": 558,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generalise_aux_verb(nlp_sent):\n",
    "    tags = nlp_sent[2]\n",
    "    words = spacy_words_to_string_array(nlp_sent[0])\n",
    "    sentence = \" \".join(words)\n",
    "    #words = \" \".join(nlp_sent[0])\n",
    "    prio_one = [['PRP', 'MD', 'VB', 'TO'], ['PRP', 'MD', 'VB', 'DT']]\n",
    "    prio_two = [['PRP', 'VBP', 'TO']]\n",
    "\n",
    "    #prio_one_patterns = [['VBP', 'VBG', 'IN'], ['MD', 'VB', 'CD'], ['VBP', 'RB']]\n",
    "    # Only when no prio_one_pattern fits\n",
    "    #prio_sec_patterns = [['VBP']]\n",
    "\n",
    "    #prio_one_patterns = [['NN', 'NNS'], ['NN', 'NN']]\n",
    "    \n",
    "    # Only when no prio_one_pattern fits\n",
    "    #prio_sec_patterns = [['NN'], ['NNS']]\n",
    "    match = False\n",
    "    extracted_words = extract_by_pattern(prio_one, tags, words)\n",
    "    if len(extracted_words) < 1:\n",
    "        extracted_words = extract_by_pattern(prio_two, tags, words)\n",
    "        #if len(extracted_words) < 1:\n",
    "        #    extracted_words = extract_by_pattern(prio_three_question_patterns, tags, words)\n",
    "    return extracted_words\n",
    "\n",
    "def extract_by_pattern(patterns, tags, words):\n",
    "    extracted_words = []\n",
    "    tags_string = \" \".join(tags)\n",
    "    for pattern in patterns:\n",
    "        \n",
    "        pattern_string = \" \".join(pattern)\n",
    "        if is_slice_in_list(pattern, tags):\n",
    "            pattern_start_index = tags.index(pattern[0])\n",
    "            if len(pattern) > 1:\n",
    "                pattern_end_index = tags.index(pattern[-1], pattern_start_index+1)\n",
    "                if pattern_end_index - pattern_start_index +1 != len(pattern):\n",
    "                    pattern_end_index = tags.index(pattern[-1], pattern_end_index+1)\n",
    "                    \n",
    "                if pattern_end_index - pattern_start_index + 1 == len(pattern):\n",
    "                    extracted_part = \"\"\n",
    "                    for i in range(pattern_start_index, pattern_end_index+1):\n",
    "                        extracted_part += words[i] + \" \"\n",
    "                        \n",
    "\n",
    "                    extracted_words.append(extracted_part[:-1])\n",
    "            else:\n",
    "                extracted_words.append(words[pattern_start_index])\n",
    "    return extracted_words\n",
    "\n",
    "def generalise_aux_verb_questions(nlp_sent):\n",
    "    tags = nlp_sent[2]\n",
    "    words = spacy_words_to_string_array(nlp_sent[0])\n",
    "    sentence = \" \".join(words)\n",
    "    #words = \" \".join(nlp_sent[0])\n",
    "    prio_one = [['MD', 'PRP', 'VB', 'DT'], ['MD', 'PRP', 'VB', 'PRP']]\n",
    "    prio_two = [['MD', 'PRP', 'VB']]\n",
    "    #prio_two = [['PRP', 'VBP', 'TO']]\n",
    "\n",
    "    #prio_one_patterns = [['VBP', 'VBG', 'IN'], ['MD', 'VB', 'CD'], ['VBP', 'RB']]\n",
    "    # Only when no prio_one_pattern fits\n",
    "    #prio_sec_patterns = [['VBP']]\n",
    "\n",
    "    #prio_one_patterns = [['NN', 'NNS'], ['NN', 'NN']]\n",
    "    \n",
    "    # Only when no prio_one_pattern fits\n",
    "    #prio_sec_patterns = [['NN'], ['NNS']]\n",
    "    match = False\n",
    "    extracted_words = extract_by_pattern(prio_one, tags, words)\n",
    "    if len(extracted_words) < 1:\n",
    "        extracted_words = extract_by_pattern(prio_two, tags, words)\n",
    "        #if len(extracted_words) < 1:\n",
    "        #    extracted_words = extract_by_pattern(prio_three_question_patterns, tags, words)\n",
    "    return extracted_words \n",
    "\n",
    "generalise_aux_verb_questions(nlp_sentence(\"would you bring me bill\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print(nlp_sentence(\"would you bring me bill\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([would, you, bring, me, bill], ['would', 'you', 'bring', 'me', 'bill'], ['MD', 'PRP', 'VB', 'PRP', 'NN'], ['MD', 'PRP', 'VB', 'PRP', 'NN'])\n",
      "['MD', 'PRP', 'VB', 'PRP']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['would you bring me']"
      ]
     },
     "execution_count": 556,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(nlp_sentence(\"would you bring me bill\"))\n",
    "generalise_aux_verb_questions(nlp_sentence(\"would you bring me bill\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n"
     ]
    }
   ],
   "source": [
    "rest_verb_questions = []\n",
    "\n",
    "for key in grammar:\n",
    "    for item in grammar[key]:\n",
    "        nlp_s = nlp_sentence(item)\n",
    "        extracted_verb_question = generalise_aux_verb_questions(nlp_s)\n",
    "        if len(extracted_verb_question) > 0:\n",
    "            rest_verb_questions.extend(extracted_verb_question)   \n",
    "\n",
    "print(len(set(rest_verb_questions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['can you tell me',\n",
       " 'can i have a',\n",
       " 'could i have s',\n",
       " 'can you give me',\n",
       " 'can i have the',\n",
       " 'can i buy a',\n",
       " 'can i pay',\n",
       " 'could i have some',\n",
       " 'could you offer me',\n",
       " 'can you bring me',\n",
       " 'could i have an',\n",
       " 'can i have s',\n",
       " 'can i find a',\n",
       " 'could i have a',\n",
       " 'can i buy',\n",
       " 'can i buy some',\n",
       " 'can i have an',\n",
       " 'could you give me',\n",
       " 'can you offer me',\n",
       " 'could i have',\n",
       " 'would you bring me',\n",
       " 'would you give me',\n",
       " 'can you show me',\n",
       " 'could i buy some',\n",
       " 'can i have',\n",
       " 'can i have some',\n",
       " 'could i buy',\n",
       " 'could you tell me',\n",
       " 'can i find the',\n",
       " 'could i buy a',\n",
       " 'could you bring me',\n",
       " 'could i have the',\n",
       " 'can i see the']"
      ]
     },
     "execution_count": 560,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(set(rest_verb_questions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['could you give', 'could i have', 'can you offer', 'can you bring', 'can you give', 'would you bring', 'could you offer', 'can i have', 'could you bring', 'can i see the']\n",
      "['dessert menu', 'bill', 'check']\n"
     ]
    }
   ],
   "source": [
    "test_prompts = [\"Frag: Ich möchte die Rechnung\", \"Frag: Ich möchte die Dessertkarte\"]\n",
    "rest_nouns= []\n",
    "rest_verb_questions = []\n",
    "for key in test_prompts:\n",
    "    for response in grammar[key]:\n",
    "        nlp_s = nlp_sentence(response)\n",
    "        extracted_nouns = extract_key_nouns_credit_card(nlp_s)\n",
    "        if len(extracted_nouns) > 0:\n",
    "            rest_nouns.extend(extracted_nouns)\n",
    "         \n",
    "\n",
    "print(list(set(rest_verb_questions)))\n",
    "print(list(set(rest_nouns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    }
   ],
   "source": [
    "found_aux_verb = []\n",
    "for key in grammar:\n",
    "    for item in grammar[key]:\n",
    "        nlp_s = nlp_sentence(item)\n",
    "        aux_verb = generalise_aux_verb(nlp_s)\n",
    "        if len(aux_verb) > 0:\n",
    "            found_aux_verb.extend(aux_verb)\n",
    "        \n",
    "        \n",
    "print(len(set(found_aux_verb)))\n",
    "#print(len(set(found_verb_prep)))\n",
    "#print(len(set(found_verb_det)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "if \"i would like the\" in \"i would like the bill\":\n",
    "    print(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i will buy these', 'i would like an', 'i will have these', 'i want to', \"i 'd need a\", 'i would like a', 'i will have the', \"i 'd like to\", 'i would like some', 'i will have an', 'i should like a', 'i will have a', \"i 'd like a\", 'i should like some', 'i need to', 'i have to', \"i 'd want a\", \"i 'd like some\", 'i will take some', 'i should like an', \"i 'd like an\", 'i should like to', 'i would need a', 'i would want a', 'i would like the', \"i 'd like the\", 'i will have some', 'i will take the', 'i will take an', 'i will take a', 'i would like to', 'i would need to']\n"
     ]
    }
   ],
   "source": [
    "print(list(set(found_aux_verb)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster No Reservation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['no i pay is there a reservation',\n",
       " '***  ***  do not have a reservation',\n",
       " '  a tomorrow at nine   ',\n",
       " 'i have no reservation',\n",
       " 'i have a   room for have a      room   ',\n",
       " \"i haven't got a reservation\",\n",
       " 'i have not a reservation',\n",
       " \"***  i'm mamma have a reservation\",\n",
       " '***  ***   a have a reservation',\n",
       " 'where no i have not have i   have no',\n",
       " \"no i don't have a reservation any\",\n",
       " 'no i  ***  ***  have a      ***    ',\n",
       " \"no i don't no i do not have a reservation\",\n",
       " 'i have no reservation',\n",
       " \"no i don't no i do not have a reservation\",\n",
       " 'no i     am good',\n",
       " 'no i have no reservation',\n",
       " 'no i    ***     am a reservation',\n",
       " 'i   ***   have a reservation',\n",
       " \"i haven't a reservation\",\n",
       " \"i don't have a reservation is bern\",\n",
       " \"i don't where is bern   \",\n",
       " 'i have a   thursday and    ',\n",
       " 'i have not a reservation',\n",
       " \"the i don't have a reservation i pay by is my name is    \",\n",
       " 'i have not a reservation',\n",
       " 'i want a reservation',\n",
       " 'i do not  ***  reservation',\n",
       " 'i have not a reservation',\n",
       " 'i have  ***  no reservation',\n",
       " \" i don't know \",\n",
       " '***  row euros hotel bar ',\n",
       " 'you are a   room ',\n",
       " 'the apple pie  ',\n",
       " 'where is there a      like to london',\n",
       " '     ***      ***  ***   ***  cirque are there a ',\n",
       " '***   can buy for the silver level',\n",
       " 'the can i    want a   soap of the silver level',\n",
       " ' do can i    got help at the silver level',\n",
       " \" i don't have a   room for the silver level\"]"
      ]
     },
     "execution_count": 562,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_prompts = [\"Sag: Ich habe keine Reservation\"]\n",
    "list(get_test_prompts_transcripts_by_cluster(test_prompts, prompt_test_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([i, have, no, reservation],\n",
       " ['i', 'have', 'no', 'reservation'],\n",
       " ['PRP', 'VBP', 'DT', 'NN'],\n",
       " ['PRP', 'VBP', 'DT', 'NN'])"
      ]
     },
     "execution_count": 563,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_sentence(\"i have no reservation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
