{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import spacy\n",
    "nlp_en = spacy.load('en')\n",
    "nlp_de = spacy.load('de')\n",
    "from spacy.en import English\n",
    "parser = English()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from spacy.en import English\n",
    "parser = English()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_prompt_response_map(gram):\n",
    "    prompt_response_map = {}\n",
    "    for prompt_unit in gram:\n",
    "        key = None\n",
    "        responses = []\n",
    "        for prompt in prompt_unit:\n",
    "            if prompt.tag == None:\n",
    "                print(\"None\")\n",
    "            else:\n",
    "                if(prompt.tag == 'prompt'):\n",
    "                    key = prompt.text\n",
    "                elif(prompt.tag == 'response'):\n",
    "                    responses.append(prompt.text)\n",
    "        prompt_response_map[key] = responses\n",
    "    return prompt_response_map\n",
    "\n",
    "def read_grammar_and_create_map(file):\n",
    "    tree = ET.parse(file)\n",
    "    grammar = tree.getroot()\n",
    "    return create_prompt_response_map(grammar)\n",
    "\n",
    "def get_sag_prompts(prompt_response_map):\n",
    "    sag_prompts = {}\n",
    "    for key in prompt_response_map:\n",
    "        if \"Sag\" in key:\n",
    "            sag_prompts[key] = prompt_response_map[key]\n",
    "    return sag_prompts\n",
    "\n",
    "    \n",
    "grammar = read_grammar_and_create_map('referenceGrammar.xml')\n",
    "#sag_prompts = get_sag_prompts(prompt_response_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_file_csv = \"textProcessing_testKaldi.csv\"\n",
    "test_data = None\n",
    "with open(test_file_csv, 'r') as reader:\n",
    "    test_data = reader.readlines()  \n",
    "    \n",
    "def prompt_map_test_data():\n",
    "    prompt_map = {}\n",
    "    for item in test_data[1:]:\n",
    "        split = item.replace(\"\\n\", \"\").split(\"\\t\")\n",
    "        prompt = split[1]\n",
    "        \n",
    "        if prompt in prompt_map:\n",
    "            prompt_map[prompt].append({'id': split[0], \"transcript\": split[3], \"prompt\": split[1]})\n",
    "        else:\n",
    "            arr = []\n",
    "            arr.append({'id':split[0], \"transcript\":split[3], \"prompt\": split[1]})\n",
    "            prompt_map[prompt] = arr\n",
    "            \n",
    "    return prompt_map\n",
    "prompt_test_map = prompt_map_test_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Basis based on Reference Grammar and Normal Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "special_case_map = {\"don't\": \"do not\",\n",
    " \"haven't\" : \"have not\",\n",
    " \"i'd\" : \"i would\",\n",
    " \"i'm\" : \"i am\",\n",
    " \"isn't\": \"is not\",\n",
    " \"it's\": \"it is\",\n",
    " \"o'clock\": \"o'clock\",\n",
    " \"that's\" :\"that is\",\n",
    " \"there's\": \"there is\"}\n",
    "\n",
    "def sentence_to_normal_case(sentence):\n",
    "    trans_sentence = \"\"\n",
    "    words = sentence.split(\" \")\n",
    "    for word in words:\n",
    "        if word in special_case_map:\n",
    "            word = special_case_map[word]\n",
    "        trans_sentence += word + \" \"\n",
    "    \n",
    "    return trans_sentence[:-1]\n",
    "\n",
    "meaning_false = {}\n",
    "for prompt_unit in prompt_test_map:\n",
    "    for response in prompt_test_map[prompt_unit]:\n",
    "        \n",
    "        sentence = response['transcript']\n",
    "        if \"***\" in sentence:\n",
    "            continue\n",
    "        if \"'\" in sentence:\n",
    "            sentence = sentence_to_normal_case(sentence)\n",
    "            #print(dict_prompt['transcript'])\n",
    "            \n",
    "        sentence = ' '.join(sentence.split())\n",
    "        if sentence not in prompt_response_map[prompt_unit]:\n",
    "            meaning_false[response['id']] = {\"sentence\":sentence, \"prompt\":response['prompt']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prompt_meaning_false = {}\n",
    "for id_ in meaning_false:\n",
    "    for item in meaning_false[id_]:\n",
    "        prompt_key = meaning_false[id_][\"prompt\"]\n",
    "        if prompt_key in prompt_meaning_false:\n",
    "            prompt_meaning_false[prompt_key].append(meaning_false[id_][\"sentence\"])\n",
    "        else:\n",
    "            arr = []\n",
    "            arr.append(meaning_false[id_][\"sentence\"])\n",
    "            prompt_meaning_false[prompt_key] = arr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Meaning Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nlp_sentence(sentence):\n",
    "    parsed = parser(sentence)\n",
    "    lemmas = []\n",
    "    words = []\n",
    "    tags = []\n",
    "    poss = []\n",
    "    for i, token in enumerate(parsed):\n",
    "        lemma = token.lemma_\n",
    "        words.append(token)\n",
    "        lemmas.append(lemma)\n",
    "        tag = token.tag_\n",
    "        pos = token.tag_\n",
    "        tags.append(tag)\n",
    "        poss.append(pos)\n",
    "    return words, lemmas, tags, poss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(meaning_false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_key = \"Sag: Ich habe keine Reservation\"\n",
    "prompt_meaning_false[test_key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def replace_sentence_emp_words(sentence):\n",
    "    return sentence.replace(\"thank you \", \"\")\\\n",
    "    .replace(\" please\", \"\")\\\n",
    "    .replace(\"yes \", \"\")\\\n",
    "    .replace(\"thank you \", \"\")\\\n",
    "    .replace(\"no \", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def spacy_words_to_string_array(words):\n",
    "    arr = []\n",
    "    for word in words:\n",
    "        arr.append(str(word))\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_key_nouns(nlp_sent):\n",
    "    tags = nlp_sent[2]\n",
    "    words = spacy_words_to_string_array(nlp_sent[0])\n",
    "    sentence = \" \".join(words) + \" \"\n",
    "    #words = \" \".join(nlp_sent[0])\n",
    "\n",
    "    tags_string = \" \".join(tags)\n",
    "    patterns = [['CD', 'JJ', 'NN'], ['JJ', 'NN'], ['DT', 'NN'], ['CD', 'JJ', 'NNS'], ['JJ', 'NNS'], ['DT', 'NNS']]\n",
    "    extracted_words = []\n",
    "    for pattern in patterns:\n",
    "        pattern_string = \" \".join(pattern) + \" \"\n",
    "        if pattern_string in tags_string: \n",
    "            if pattern[-1] not in tags and pattern[0] not in tags:\n",
    "                continue\n",
    "            pattern_start_index = tags.index(pattern[0])\n",
    "            pattern_end_index = tags.index(pattern[-1])\n",
    "            if pattern_end_index - pattern_start_index + 1 == len(pattern):\n",
    "                extracted_part = \"\"\n",
    "                for i in range(pattern_start_index, pattern_end_index+1):\n",
    "                    extracted_part += words[i] + \" \"\n",
    "                \n",
    "                extracted_words.append(extracted_part[:-1])\n",
    "    return extracted_words\n",
    "\n",
    "#print(nlp_sentence(\"i am looking for one musical ticket\")[2])    \n",
    "extract_key_nouns(nlp_sentence(\"i am looking for one musical ticket\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "counter = 0\n",
    "result = []\n",
    "for sentence in grammar[test_key]:\n",
    "    replaced_sentence = replace_sentence_emp_words(sentence)\n",
    "    nlp_sent = nlp_sentence(replaced_sentence)\n",
    "    tags = nlp_sent[2]\n",
    "    if tags[0] == 'PRP':\n",
    "        counter +=1\n",
    "        result.extend(extract_key_nouns(nlp_sent))\n",
    "        print(str(spacy_words_to_string_array(nlp_sent[0])))\n",
    "        print(nlp_sent[2])\n",
    "print(counter)\n",
    "print(len(result))\n",
    "print(set(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_key_verbs_by_prp(nlp_sent):\n",
    "    tags = nlp_sent[2]\n",
    "    words = spacy_words_to_string_array(nlp_sent[0])\n",
    "    sentence = \" \".join(words)\n",
    "    #words = \" \".join(nlp_sent[0])\n",
    "\n",
    "    tags_string = \" \".join(tags)\n",
    "    prio_one_patterns = [['VBP', 'VBG', 'IN'], ['MD', 'VB', 'CD'], ['VBP', 'RB']]\n",
    "    # Only when no prio_one_pattern fits\n",
    "    prio_sec_patterns = [['VBP']]\n",
    "    match = False\n",
    "    extracted_words = []\n",
    "    for pattern in prio_one_patterns:\n",
    "        pattern_string = \" \".join(pattern)\n",
    "        if pattern_string in tags_string:  \n",
    "            pattern_start_index = tags.index(pattern[0])\n",
    "            pattern_end_index = tags.index(pattern[-1])\n",
    "            if pattern_end_index - pattern_start_index + 1 == len(pattern):\n",
    "                extracted_part = \"\"\n",
    "                for i in range(pattern_start_index, pattern_end_index+1):\n",
    "                    extracted_part += words[i] + \" \"\n",
    "                \n",
    "                extracted_words.append(extracted_part[:-1])\n",
    "    if len(extracted_words) <1:\n",
    "        for pattern in prio_sec_patterns:\n",
    "            pattern_string = \" \".join(pattern)\n",
    "            if pattern_string in tags_string:  \n",
    "                pattern_start_index = tags.index(pattern[0])\n",
    "                pattern_end_index = tags.index(pattern[-1])\n",
    "                if pattern_end_index - pattern_start_index + 1 == len(pattern):\n",
    "                    extracted_part = \"\"\n",
    "                    for i in range(pattern_start_index, pattern_end_index+1):\n",
    "                        extracted_part += words[i] + \" \"\n",
    "\n",
    "                    extracted_words.append(extracted_part[:-1])\n",
    "        \n",
    "    return extracted_words\n",
    "\n",
    "extract_key_verbs_by_prp(nlp_sentence(\"i am looking for one musical ticket\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_key_questions(nlp_sent):\n",
    "    tags = nlp_sent[2]\n",
    "    words = spacy_words_to_string_array(nlp_sent[0])\n",
    "    sentence = \" \".join(words)\n",
    "    #words = \" \".join(nlp_sent[0])\n",
    "\n",
    "    tags_string = \" \".join(tags)\n",
    "    prio_one_patterns = [['MD', 'PRP', 'VB'], ['VBP', 'PRP', 'VBN'], ['VBP', 'PRP', 'VB']]\n",
    "    # Only when no prio_one_pattern fits\n",
    "    #prio_sec_patterns = [['VBP']]\n",
    "    match = False\n",
    "    extracted_words = []\n",
    "    for pattern in prio_one_patterns:\n",
    "        pattern_string = \" \".join(pattern) + \" \"\n",
    "        if pattern_string in tags_string:\n",
    "            if pattern[0] not in tags and pattern[-1] not in tags:\n",
    "                continue\n",
    "            pattern_start_index = tags.index(pattern[0])\n",
    "            pattern_end_index = tags.index(pattern[-1])\n",
    "            if pattern_end_index - pattern_start_index + 1 == len(pattern):\n",
    "                extracted_part = \"\"\n",
    "                for i in range(pattern_start_index, pattern_end_index+1):\n",
    "                    extracted_part += words[i] + \" \"\n",
    "                \n",
    "                extracted_words.append(extracted_part[:-1])\n",
    "        \n",
    "    return extracted_words\n",
    "\n",
    "_test_key = \"have you got one musical ticket\"\n",
    "#print(nlp_sentence(_test_key)[2])\n",
    "extract_key_questions(nlp_sentence(_test_key))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "counter = 0\n",
    "result_prp = []\n",
    "result_md = []\n",
    "for sentence in grammar[test_key]:\n",
    "    replaced_sentence = replace_sentence_emp_words(sentence)\n",
    "    nlp_sent = nlp_sentence(replaced_sentence)\n",
    "    tags = nlp_sent[2]\n",
    "    if tags[0] != 'PRP':\n",
    "        extracted = extract_key_questions(nlp_sent)\n",
    "        if len(extracted) < 1:\n",
    "            print(sentence)\n",
    "            print(nlp_sent[2])\n",
    "            #print(extract_key_verbs_by_prp(nlp_sent))\n",
    "        else:\n",
    "            result_md.extend(extracted)\n",
    "            \n",
    "\n",
    "        #print(str(spacy_words_to_string_array(nlp_sent[0])))\n",
    "        #print(nlp_sent[2])\n",
    "print(counter)\n",
    "print(len(result_prp))\n",
    "print(set(result_md))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine NN, Q, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "extract_key_nouns(nlp_sentence(\"i do not have a reservation\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for sent in errors_nouns:\n",
    "    n_s = nlp_sentence(sent)\n",
    "    try:    \n",
    "        extract_key_nouns(n_s)\n",
    "    except:\n",
    "        print(sent)\n",
    "        print(n_s)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "verbs = []\n",
    "nouns = []\n",
    "errors_nouns = []\n",
    "errors_verbs = []\n",
    "for key in grammar:\n",
    "    for response in grammar[key]:\n",
    "        replaced_sentence = replace_sentence_emp_words(response)\n",
    "        nlp_sent = nlp_sentence(replaced_sentence)  \n",
    "        tags = nlp_sent[2]\n",
    "        if tags[0] == 'PRP':\n",
    "            try:\n",
    "                verbs.extend(extract_key_verbs_by_prp(nlp_sent))\n",
    "            except:\n",
    "                errors_verbs.append(response)\n",
    "            try:    \n",
    "                nouns.extend(extract_key_nouns(nlp_sent))\n",
    "            except:\n",
    "                errors_nouns.append(response)\n",
    "            \n",
    "        \n",
    "print(len(set(verbs)))\n",
    "print(len(set(nouns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for verb in set(verbs):\n",
    "    print(verb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "verbs = []\n",
    "nouns = []\n",
    "for response in grammar[test_key]:\n",
    "    replaced_sentence = replace_sentence_emp_words(response)\n",
    "    nlp_sent = nlp_sentence(replaced_sentence)  \n",
    "    tags = nlp_sent[2]\n",
    "    if tags[0] == 'PRP':\n",
    "        verbs.extend(extract_key_verbs_by_prp(nlp_sent))\n",
    "        nouns.extend(extract_key_nouns(nlp_sent))\n",
    "        \n",
    "        \n",
    "print(set(verbs))\n",
    "print(set(nouns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for item in prompt_test_map[test_key]:\n",
    "    if item['transcript'] not in grammar[test_key]:\n",
    "        print(item['transcript'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sag_prompts = get_sag_prompts(grammar)\n",
    "for key in sag_prompts:\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_prompts = [\"Sag: Ich möchte mit Dollars bezahlen\",\n",
    "\"Sag: Ich möchte mit Euros bezahlen\",\n",
    "\"Sag: Ich möchte mit Kreditkarte bezahlen\",\n",
    "\"Sag: Ich möchte mit Mastercard bezahlen\",\n",
    "\"Sag: Ich möchte mit Postkarte bezahlen\",\n",
    "\"Sag: Ich möchte mit Visa bezahlen\",\n",
    "\"Sag: Ich möchte mit Kreditkarte bezahlen\",\n",
    "\"Sag: Ich möchte mit Mastercard bezahlen\",\n",
    "\"Sag: Ich möchte mit Postkarte bezahlen\",\n",
    "\"Sag: Ich möchte mit Visa bezahlen\",\n",
    "\"Sag: Ich möchte mit Dollars bezahlen\",\n",
    "\"Sag: Ich möchte mit Euros bezahlen\",\n",
    "\"Sag: Ich möchte mit Kreditkarte bezahlen\",\n",
    "\"Sag: Ich möchte mit Mastercard bezahlen\",\n",
    "\"Sag: Ich möchte mit Postkarte bezahlen\",\n",
    "\"Sag: Ich möchte mit Visa bezahlen\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_prompts_sentences = []\n",
    "counter_max = 0\n",
    "max_prompt = \"\"\n",
    "for test_prompt in test_prompts:\n",
    "    tp_prompts = grammar[test_prompt]\n",
    "    if len(tp_prompts) > counter_max:\n",
    "        max_prompt = test_prompt\n",
    "        counter_max = len(tp_prompts)\n",
    "        \n",
    "    test_prompts_sentences.extend(tp_prompts)\n",
    "print(len(test_prompts_sentences))\n",
    "print(counter_max)\n",
    "print(max_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_slice_in_list(s,l):\n",
    "    len_s = len(s) #so we don't recompute length of s on every iteration\n",
    "    return any(s == l[i:len_s+i] for i in range(len(l) - len_s+1))\n",
    "\n",
    "def extract_by_pattern(patterns, tags, words):\n",
    "    extracted_words = []\n",
    "    tags_string = \" \".join(tags)\n",
    "    for pattern in patterns:\n",
    "        pattern_string = \" \".join(pattern)\n",
    "        if is_slice_in_list(pattern, tags):\n",
    "            pattern_start_index = tags.index(pattern[0])\n",
    "            if len(pattern) > 1:\n",
    "                pattern_end_index = tags.index(pattern[-1], pattern_start_index+1)\n",
    "                if pattern_end_index - pattern_start_index + 1 == len(pattern):\n",
    "                    extracted_part = \"\"\n",
    "                    for i in range(pattern_start_index, pattern_end_index+1):\n",
    "                        extracted_part += words[i] + \" \"\n",
    "\n",
    "                    extracted_words.append(extracted_part[:-1])\n",
    "            else:\n",
    "                extracted_words.append(words[pattern_start_index])\n",
    "    return extracted_words\n",
    "    \n",
    "def extract_key_nouns_credit_card(nlp_sent):\n",
    "    tags = nlp_sent[2]\n",
    "    words = spacy_words_to_string_array(nlp_sent[0])\n",
    "    sentence = \" \".join(words)\n",
    "    #words = \" \".join(nlp_sent[0])\n",
    "\n",
    "    prio_one_patterns = [['NN', 'NNS'], ['NN', 'NN']]\n",
    "    \n",
    "    # Only when no prio_one_pattern fits\n",
    "    prio_sec_patterns = [['NN'], ['NNS']]\n",
    "    match = False\n",
    "    extracted_words = extract_by_pattern(prio_one_patterns, tags, words)\n",
    "    if len(extracted_words) < 1:\n",
    "        extracted_words = extract_by_pattern(prio_sec_patterns, tags, words)\n",
    "    \n",
    "    return extracted_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([can, i, pay, by, credit, card], ['can', 'i', 'pay', 'by', 'credit', 'card'], ['MD', 'PRP', 'VB', 'RP', 'NN', 'NN'], ['MD', 'PRP', 'VB', 'RP', 'NN', 'NN'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['credit card']"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testtt = \"can i pay by credit card\"\n",
    "print(nlp_sentence(testtt))\n",
    "extract_key_nouns_credit_card(nlp_sentence(testtt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sag: Ich möchte mit Dollars bezahlen\n",
      "Sag: Ich möchte mit Euros bezahlen\n",
      "Sag: Ich möchte mit Kreditkarte bezahlen\n",
      "Sag: Ich möchte mit Mastercard bezahlen\n",
      "Sag: Ich möchte mit Postkarte bezahlen\n",
      "Sag: Ich möchte mit Visa bezahlen\n",
      "Sag: Ich möchte mit Kreditkarte bezahlen\n",
      "Sag: Ich möchte mit Mastercard bezahlen\n",
      "Sag: Ich möchte mit Postkarte bezahlen\n",
      "Sag: Ich möchte mit Visa bezahlen\n",
      "Sag: Ich möchte mit Dollars bezahlen\n",
      "Sag: Ich möchte mit Euros bezahlen\n",
      "Sag: Ich möchte mit Kreditkarte bezahlen\n",
      "Sag: Ich möchte mit Mastercard bezahlen\n",
      "Sag: Ich möchte mit Postkarte bezahlen\n",
      "Sag: Ich möchte mit Visa bezahlen\n",
      "{'card', 'credit card', 'mastercard', 'visa', 'dollars', 'cards', 'master card', 'euros', 'credit cards', 'post card'}\n"
     ]
    }
   ],
   "source": [
    "test_key = \"Sag: Ich möchte mit Kreditkarte bezahlen\"\n",
    "\n",
    "nouns = []\n",
    "for key in test_prompts:\n",
    "    print(key)\n",
    "    for item in grammar[key]:\n",
    "        nlp_se = nlp_sentence(item)\n",
    "        nouns.extend(extract_key_nouns_credit_card(nlp_se))\n",
    "print(set(nouns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
